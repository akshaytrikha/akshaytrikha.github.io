<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>(Bitter) Lessons from Scaling Machine Learning Interatomic Potentials | Akshay Trikha</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="(Bitter) Lessons from Scaling Machine Learning Interatomic Potentials" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/deep-learning/2025/09/25/scaling.html" />
<meta property="og:url" content="http://localhost:4000/deep-learning/2025/09/25/scaling.html" />
<meta property="og:site_name" content="Akshay Trikha" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-09-25T06:11:17-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="(Bitter) Lessons from Scaling Machine Learning Interatomic Potentials" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-09-25T06:11:17-07:00","datePublished":"2025-09-25T06:11:17-07:00","headline":"(Bitter) Lessons from Scaling Machine Learning Interatomic Potentials","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/deep-learning/2025/09/25/scaling.html"},"url":"http://localhost:4000/deep-learning/2025/09/25/scaling.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Akshay Trikha" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Akshay Trikha</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/projects/">Projects</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">(Bitter) Lessons from Scaling Machine Learning Interatomic Potentials</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-09-25T06:11:17-07:00" itemprop="datePublished">Sep 25, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<p><em>Or, Why My Master’s Thesis Failed</em></p>

<p><strong>TL;DR:</strong> I tested whether a plain transformer encoder (no equivariance, no periodic boundary conditions) can learn interatomic forces as data &amp; params scale vs. EquiformerV2. EqV2 followed clean power laws and hit much lower loss. My transformer failed and plateaued hard, leading me to believe my featurization was insufficient. I share the wins &amp; failures.</p>

<p>For EquiformerV2:</p>

\[\begin{aligned}
&amp;\quad\text{Parameter scaling law:} &amp;&amp;L \approx 7.76\times 10^{2}\, P^{-0.383} \\
&amp;\quad\text{Compute scaling law:} &amp;&amp;L \approx 4.99\times 10^{5}\, C^{-0.339} \\
&amp;\quad\text{Data scaling law:} &amp;&amp;L \approx 6.47\times 10^{1}\, D^{-0.242}
\end{aligned}\]

<p>Code is open sourced <a href="https://github.com/akshaytrikha/materials-scaling">here</a>.</p>

<hr />
<p><br />
A little over a year ago I got pretty interested in training neural networks that can learn physics from data. Around the time machine learning interatomic potential (NNIP) models were becoming popular. These models broadly take as input a material’s atomic configuration and predict properties related to its potential energy. All the papers I was reading had great results, but I felt they were selling an incomplete story because they were missing scaling information. To me, understanding how a model scales is perhaps the most important factor and having just 1 datapoint of a final test loss was insufficient to understand how any architecture would stand the test of time.</p>

<p>I tried to investigate whether vanilla transformer encoders, given sufficient data, could learn to predict material properties as well as architectures explicitly equivariant architectures, like EquiformerV2 (EqV2) <a href="https://arxiv.org/pdf/2306.12059">[1]</a>. Inspired by the Bitter Lesson <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">[2]</a>, my hypothesis was that that transformers would scale more slowly than specialized architectures but that their scaling laws <a href="https://arxiv.org/pdf/2001.08361">[3]</a><a href="https://arxiv.org/pdf/1712.00409">[4]</a><a href="https://arxiv.org/pdf/2210.16859">[5]</a> would hold out over more orders-of-magnitude (OOM).</p>

<p>I failed to stably train transformers most likely due to the featurization I chose. I still learned a great deal along the way, and found power laws for EqV2.</p>

<p><br />
<strong>Why the math makes sense:</strong></p>

<p>A Transformer is a graph neural network (GNN) on a complete graph with learned edge weights <a href="https://arxiv.org/pdf/1704.01212">[6]</a>. A graph in a GNN is created through a rule, either a known relation between nodes i.e. this paper cites another or a cutoff i.e. this atom is too far from the other so we assume they won’t interact.</p>

<iframe id="attentionFrame" src="http://localhost:4000/assets/scaling/attention-graph.html" width="100%" style="border:0; border-radius:8px; box-shadow:0 2px 8px rgba(0,0,0,.1); background:#fff"></iframe>
<script>
  const f = document.getElementById('attentionFrame');
  f.addEventListener('load', () => {
    const doc = f.contentDocument || f.contentWindow.document;
    f.style.height = doc.documentElement.scrollHeight + 'px';
  });
</script>

<p><br />
A simple argument between the two architectures is that they fall into the bias - expressivity tradeoff. My take is that since self-attention on a fully connected graph is mathematically equivalent to message passing <a href="https://arxiv.org/pdf/1704.01212">[6]</a> it should be able to learn weights between atoms without having to describe them.</p>

<p><br />
<strong>Task and Dataset</strong></p>

<p>MLIPs are trained to take in a set of atoms and their positions to predict the structure’s energy and typically the forces on the atoms as well as the structure’s stresses.</p>

<p>A common criticism of NNIPs trained on Density Functional Theory (DFT) calculations is that those datasets are relaxed structures around 0K. This means that they’re not physically relevant in most cases to us because we don’t live around 0K.</p>

<p>The Open Materials 2024 (OMat24) <a href="https://huggingface.co/datasets/facebook/OMAT24">[7]</a> dataset is a 110 million crystal structure dataset that addresses this problem by focusing on including non-equilibrium (high temperature, structurally perturbed) samples. It’s also one of the largest datasets of its kind.</p>

<hr />

<p><br />
<strong>Sin #1: Transformer featurization</strong></p>

<p>In my attempt at training transformers I wanted to use as few inductive biases as possible. i.e. no equivariant features, no invariant features, no periodic boundary conditions. This was an attempt to learn everything from the data + augmentation, regardless of sample efficiency.</p>

<p>I used a standard embedding layer for atom types to give the model a dictionary lookup of what each atom is across different structures. This was important because the model needed to understand that each atom is the same in different structures but is modified by its context, similar to how each word is the same in different sequences and is modified by its context. The 3D positions were concatenated with the embedding vector because I thought the model might have an easier time disentangling meaning vs. saving parameters by adding the positions to the embeddings.</p>

<!-- $$
\begin{align}
&\text{Input feature matrix: } \left[
\begin{array}{ccc|ccc}
e_{11} & \cdots & e_{1d} & x_1 & y_1 & z_1 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
e_{N1} & \cdots & e_{Nd} & x_N & y_N & z_N
\end{array}
\right]
\in \mathbb{R}^{N\times(d+3)}
\end{align}
$$ -->

\[\begin{array}{c@{}c@{}c}
\text{Input feature matrix: } &amp;
\left[
\begin{array}{ccc|ccc}
e_{11} &amp; \cdots &amp; e_{1d} &amp; x_1 &amp; y_1 &amp; z_1 \\
\vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
e_{N1} &amp; \cdots &amp; e_{Nd} &amp; x_N &amp; y_N &amp; z_N
\end{array}
\right] &amp;
\in \mathbb{R}^{N\times(d+3)}
\\[-2ex]
&amp; \begin{array}{c@{\mkern8mu}c}
\underbrace{\hphantom{e_{11}\ \cdots\ e_{1d}}}_{\text{atomic embeddings}} &amp;
\underbrace{\hphantom{x_1\ y_1\ z_1}}_{\text{positional encoding}}
\end{array}
\end{array}\]

<p>This was purposefully not a rotationally invariant featurization as I wanted to see if the model could learn this through augmentation. It also did not account for the fact crystal structures are periodic, which means that forces on atoms can come from adjacent cells. My findings are that this featurization led to the model learning global structure energy and stresses well, but not 3D per-atom forces. This isn’t to say that forces weren’t learned at all, but they were certainly not comparable to EquiformerV2.</p>

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="http://localhost:4000/assets/scaling/train-force-loss.png" alt="transformer train force loss sweep" />
        <figcaption>Sweeping 1M parameter transformers on 100k structures</figcaption>
    </div>
    <br />
</figure>

<p>I got stuck with this phenomena of an apparent plateau in force loss only for certain hyperparam configs. There were other runs that would break through but still plateaued much higher than the EqV2. I suspect that training stability had a role with the former, and ultimately the lack of periodic boundary conditions impacting the latter.</p>

<p>Tough lessons learned:</p>
<ul>
  <li>The models just want to learn and given enough parameters the loss will go down even if they aren’t learning the right thing.</li>
  <li>If scaling behavior doesn’t appear in smaller OOMs it’s unlikely it will magically appear later</li>
</ul>

<p><br />
<strong>Sin #2: Not starting with individual experiments</strong></p>

<p>Instead, I rushed to create a more complex <a href="https://github.com/akshaytrikha/materials-scaling">set of scripts</a> that would automatically run scaling experiments over multiple OOMs. I came up with what I thought was a clever way of iterating through models:</p>

<!-- ```Python
class MetaTransformerModels:
    def __init__(
        self,
        vocab_size,
        max_seq_len,
    ):
        """Initializes TransformerModels with a list of configurations"""
        self.configurations = [
            {"d_model": 2, "depth": 1, "n_heads": 1, "d_ff_mult": 4}, # 2,280 params
            ...
            {"d_model": 256, "depth": 3, "n_heads": 2, "d_ff_mult": 4}, # 2,414,000 params
        ]
        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len

    def __getitem__(self, idx):
        """Retrieves transformer model corresponding to the configuration at idx"""
        config = self.configurations[idx]

        return XTransformerModel(
            num_tokens=self.vocab_size,
            d_model=config["d_model"],
            depth=config["depth"],
            n_heads=config["n_heads"],
            d_ff_mult=config["d_ff_mult"],
        )

    def __len__(self):
        return len(self.configurations)

    def __iter__(self):
        for idx in range(len(self.configurations)):
            yield self[idx]
``` -->

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="http://localhost:4000/assets/scaling/MetaTransformerModels.png" alt="MetaTransformerModels" />
    </div>
    <br />
</figure>

<p>On paper this sounds great to iterate over model sizes and lazily instantiate them, but in practice each OOM brings nuance and unexpectedness in behavior through e.g. hyperparameter sensitivity like early stoping. Instead, I should’ve started with small manual overfitting experiments and gradually increasing the parameters and data <a href="http://karpathy.github.io/2019/04/25/recipe/">[8]</a>.</p>

<p><br />
<strong>Sin #3: Not starting with a small, in-memory, dataset</strong></p>

<p>Over all the experiments I ran I found that dataloading was typically the bottleneck in training time. This is an example of a bad dataset class I wrote. The devil is in the details because the <code class="language-plaintext highlighter-rouge">AseDBDataset.get_atoms(idx)</code> call looks like a simple getter but is actually doing disk I/O.</p>

<div style="display: grid; grid-template-columns: 1.2fr 1fr; gap: 20px; margin: 20px 0;">
<style>
  .code-comparison pre,
  .code-comparison code {
    font-size: 0.85em;
  }
</style>
<div class="code-comparison">

    <p><strong>Bad approach ❌</strong></p>

    <p><img src="http://localhost:4000/assets/scaling/bad-approach.png" alt="Bad approach code" /></p>

    <p>The result is that all of this work repeats every call:</p>
    <ul>
      <li>With random shuffling causing worst-case random disk access</li>
      <li>Across multiple worker processes (each with their own DB connections)</li>
    </ul>

    <p>This is painful.</p>

  </div>
<div class="code-comparison">

    <p><strong>Better approach ✅</strong></p>

    <p><img src="http://localhost:4000/assets/scaling/better-approach.png" alt="Better approach code" /></p>

    <p>A very simple solution is to start experimenting with a very small dataset and iterate through it completely to cache it before training.</p>

  </div>
</div>

<p>At a small dataset scale the first approach didn’t matter. But it wasted a lot of time as I scaled the dataset size.</p>

<hr />

<p><br />
<strong>Win #1: Some EquiformerV2 results</strong></p>

<div class="eqv2-results-container">
  <div class="result-img">
    <img src="http://localhost:4000/assets/scaling/eqv2-param-scaling.png" alt="EquiformerV2 Parameter Scaling" />
  </div>
  <div class="result-img">
    <img src="http://localhost:4000/assets/scaling/eqv2-compute-scaling.png" alt="EquiformerV2 Compute Scaling" />
  </div>
  <div class="result-img">
    <img src="http://localhost:4000/assets/scaling/eqv2-dataset-scaling.png" alt="EquiformerV2 Dataset Scaling" />
  </div>
</div>

<style>
.eqv2-results-container{
  /* full width breakout */
  width: 100vw;
  max-width: 100vw;
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);

  /* 3-up grid */
  padding-inline: 2rem;           /* optional gutter */
  box-sizing: border-box;
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 2rem;
  align-items: stretch;
}

.result-img{ 
  width:100%; 
  margin:0;
  display: flex;
  align-items: center;
  justify-content: center;
}

.result-img img{
  width: 100%;
  height: 350px;
  object-fit: contain;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

/* responsive wrap-downs */
@media (max-width: 1200px){ .eqv2-results-container{ grid-template-columns: repeat(2, 1fr); } }
@media (max-width: 768px){  .eqv2-results-container{ grid-template-columns: 1fr; } }
</style>

<p><br />
Takeaways:</p>
<ul>
  <li>Demonstrated power laws</li>
  <li>Data scaling shows diminishing returns compared to param scaling</li>
  <li>Identified compute-optimal training configs with a real pareto frontier</li>
  <li>Data and training pipeline worked for EqV2, so there was something inherently wrong with the transformer</li>
</ul>

<p><br />
<strong>Win #2: Making an inference visualization tool</strong></p>

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="http://localhost:4000/assets/scaling/inference.gif" alt="inference visualization" />
    </div>
    <br />
</figure>

<p>This ended up being a very useful debugging tool to understand how the models were behaving. For example as a sanity check it was nice to see the transformer wasn’t only predicting 0 forces nor the mean of the dataset, and also that models were learning smaller magnitude forces as well as larger ones.</p>

<p><br />
<strong>Win #3: Making a scaling law experiment run tool</strong></p>

<div class="scaling-plots-container">
  <div id="eqv2-scaling-plot1" class="plot-main"></div>
  <div class="plot-dual-container">
    <div id="eqv2-scaling-plot2" class="plot-secondary"></div>
    <div id="eqv2-scaling-plot3" class="plot-secondary"></div>
  </div>
</div>

<style>
.scaling-plots-container{
  /* full width breakout */
  width: 100vw;
  max-width: 100vw;
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);

  /* keep your 3-up grid */
  padding-inline: 2rem;           /* optional gutter */
  box-sizing: border-box;
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 2rem;
  align-items: stretch;
}

/* keep these from Option A */
.plot-dual-container{ display: contents; }
.plot-main, .plot-secondary{ width:100%; height:400px; margin:0; }

/* responsive wrap-downs */
@media (max-width: 1200px){ .scaling-plots-container{ grid-template-columns: repeat(2, 1fr); } }
@media (max-width: 768px){  .scaling-plots-container{ grid-template-columns: 1fr; } }
</style>

<!-- Plotly -->
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>

<!-- Load your module and render -->
<script type="module">
  import { renderEqV2Scaling } from "/assets/scaling/eqv2-scaling.js";
  renderEqV2Scaling('eqv2-scaling-plot1', 'eqv2-scaling-plot2', 'eqv2-scaling-plot3');
</script>

<p>This tool helped group families of runs and studying their behavior on the same plot. Before training any models I established naive (no deep learning) baselines that would help understand the loss number. The three baselines were:</p>
<ul>
  <li>Loss while predicting 0 everywhere</li>
  <li>Loss while predicting the mean of the dataset everywhere</li>
  <li>Loss from a k = 1 nearest-neighbor Markov model</li>
</ul>

<p><br />
<strong>Win #4: Getting to talk to John Jumper about scaling AlphaFold</strong></p>

<p>I think my life peaked a little when I got to have a pizza and a beer with John Jumper at the YC AI Startup School and pitch him this unfinished research.</p>

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="http://localhost:4000/assets/scaling/john-jumper.jpg" alt="meeting John Jumper" />
        <figcaption>I met one of my heroes and he turned out to be extremely kind.</figcaption>
    </div>
    <br />
</figure>

<!-- A Dummy’s Guide to Empirically Deriving Scaling Laws

(& What not to do)

1. Source compute first (assuming you already have a large dataset)
2. Do NOT start by YOLOing a huge run
    - Each run is only 1 data point so a large one is still only 1 data point
    - Stay small as long as possible
3. Your first goal is to overfit a model. Do this by following http://karpathy.github.io/2019/04/25/recipe/
4. Don’t forget to visualize & verify your outputs
5. Training efficiency bells & whistles
    1. FlashAttention, mixed precision gradient clipping,
-->

<h4 id="references">References:</h4>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2306.12059">EquiformerV2</a> - Liao et al., 2023</li>
  <li>[2] <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">The Bitter Lesson</a> - Richard Sutton, 2019</li>
  <li>[3] <a href="https://arxiv.org/pdf/2001.08361">Scaling Laws for Neural Language Models</a> - Kaplan et al., 2020</li>
  <li>[4] <a href="https://arxiv.org/pdf/1712.00409">Deep Learning Scaling Is Predictable, Empirically</a> - Hestness et al., 2017</li>
  <li>[5] <a href="https://arxiv.org/pdf/2210.16859">A Solvable Model of Neural Scaling Laws</a> - Maloney et al., 2022</li>
  <li>[6] <a href="https://arxiv.org/pdf/1704.01212">Neural Message Passing for Quantum Chemistry</a> - Gilmer et al., 2017</li>
  <li>[7] <a href="https://huggingface.co/datasets/facebook/OMAT24">Open Materials 2024 (OMat24) Dataset</a> - Meta AI, 2024</li>
  <li>[8] <a href="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks</a> - Andrej Karpathy, 2019</li>
  <li><a href="https://arxiv.org/pdf/2203.15556">Training Compute-Optimal Large Language Models (Chinchilla)</a> - Hoffmann et al., 2022</li>
  <li><a href="https://distill.pub/2020/circuits/equivariance/">Naturally Occurring Equivariance in Neural Networks</a> - Olah et al., 2020</li>
</ul>

  </div><a class="u-url" href="/deep-learning/2025/09/25/scaling.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/"></data>
  
    <div class="wrapper">
  
      <div class="footer-col-wrapper" style="margin-top: -20px;">
        <div class="footer-col">
        </div>
        <div class="footer-col">
          <p></p>
        </div>
      </div>
  
      <div class="social-links"><ul class="social-media-list"><li><a href="https://github.com/akshaytrikha"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">akshaytrikha</span></a></li><li><a href="https://www.linkedin.com/in/akshay-trikha"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">akshay-trikha</span></a></li></ul>
</div>
  
    </div>
</footer>
  </body>

</html>
