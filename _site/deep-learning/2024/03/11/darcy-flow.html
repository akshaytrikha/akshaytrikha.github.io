<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Solving Darcy Flow with Neural PDEs | Akshay Trikha</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Solving Darcy Flow with Neural PDEs" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html" />
<meta property="og:url" content="http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html" />
<meta property="og:site_name" content="Akshay Trikha" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-03-11T06:11:17-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Solving Darcy Flow with Neural PDEs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-03-11T06:11:17-07:00","datePublished":"2024-03-11T06:11:17-07:00","headline":"Solving Darcy Flow with Neural PDEs","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html"},"url":"http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Akshay Trikha" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Akshay Trikha</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/">Blog</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Solving Darcy Flow with Neural PDEs</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-03-11T06:11:17-07:00" itemprop="datePublished">Mar 11, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<p>How do we force a neural network to only predict functions that satisfy some physical contraint? Let’s take the example of 2D darcy flow, a partial differential equation (PDE) that describes the flow of a fluid through a porous medium. Darcy flow states:</p>

\[-\nabla \cdot \left( \nu(x,y) \nabla u(x,y) \right) = f(x,y) = \quad x,y \in (0,1)\]

<p>With boundary condition:</p>

\[u(x,y) = 0 \quad (x,y) \in \partial(0,1)^2\]

<p>Ok great, we have a differential equation and our goal is to train a neural network that guesses solutions to this. In our heads, we imagine our solutions to be of the form \(u = \sigma_i k_i w_i\) which is a product of basis functions $k$ and their corresponding weights $w$. Now we see a problem - we need to optimize two things with this approach: we need to predict optimal basis functions in addition to optimal weights for our final prediction to be optimal. This type of problem is called bi-level optimization and we’re gonna solve it with wizardry (or at least I think so).</p>

<!-- \mathcal{F}(u)  -->

<p>We’re going to implmenet a differentiable optimization layer (I know, crazy) that will allow us to</p>

  </div><a class="u-url" href="/deep-learning/2024/03/11/darcy-flow.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/"></data>
  
    <div class="wrapper">
  
      <div class="footer-col-wrapper" style="margin-top: -20px;">
        <div class="footer-col">
        </div>
        <div class="footer-col">
          <p></p>
        </div>
      </div>
  
      <div class="social-links"><ul class="social-media-list"><li><a href="https://github.com/akshaytrikha"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">akshaytrikha</span></a></li><li><a href="https://www.linkedin.com/in/akshay-trikha"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">akshay-trikha</span></a></li></ul>
</div>
  
    </div>
</footer>
  </body>

</html>
