<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Solving Darcy Flow with Neural PDEs | Akshay Trikha</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Solving Darcy Flow with Neural PDEs" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html" />
<meta property="og:url" content="http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html" />
<meta property="og:site_name" content="Akshay Trikha" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-03-11T06:11:17-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Solving Darcy Flow with Neural PDEs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-03-11T06:11:17-07:00","datePublished":"2024-03-11T06:11:17-07:00","headline":"Solving Darcy Flow with Neural PDEs","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html"},"url":"http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Akshay Trikha" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Akshay Trikha</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/">Blog</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Solving Darcy Flow with Neural PDEs</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2024-03-11T06:11:17-07:00" itemprop="datePublished">Mar 11, 2024
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<p><strong>Background</strong></p>

<p>How do we force a neural network to only predict functions that satisfy some physical contraint? Let’s take the example of 2D darcy flow, a partial differential equation (PDE) that describes the flow of a fluid through a porous medium. Darcy flow states:</p>

<!-- $$ -\nabla \cdot \left( \nu(x,y) \nabla u(x,y) \right) = f(x,y) \quad \quad \quad \quad x,y \in (0,1) \quad (1)$$ -->

<!-- With boundary condition: -->

<!-- $$ u(x,y) = 0 \quad \quad \quad \quad (x,y) \in \partial(0,1)^2 $$ -->

<p>\begin{align}
    -\nabla \cdot \left( \nu(x,y) \nabla u(x,y) \right) &amp;= f(x,y) &amp; x,y &amp;\in (0,1) &amp; \tag{1} \newline
\end{align}</p>

<p>With boundary condition</p>

<p>\begin{align}
    u(x,y) &amp;= 0 &amp; (x,y) &amp;\in \partial(0,1)^2 \tag{2}
\end{align}</p>

<p>Where</p>

<p>\begin{align}
    \nabla &amp;= \text{divergence operator} \newline
    \nu &amp;= \text{diffusion coefficient} \newline
    u &amp;= \text{potential} \newline
    f(x,y) &amp;= \text{forcing function} = 1 \quad \forall x,y
\end{align}</p>

<p>Ok great, we have a differential equation and our goal is to train a neural network that guesses solutions to this. In our heads, we imagine our solutions could be expressed as a linear combination of basis functions and their corresponding weights</p>

\[u = \Sigma_i k_i w_i \tag{3}\]

<p>Now we see a problem - we need to optimize two things with this approach: we need to predict optimal basis functions in addition to optimal weights for our final prediction to be good. This type of problem is called bi-level optimization and we’re gonna solve it with wizardry (or at least I think so). So in this ideal world our neural network is going to predict some basis functions and then we’re gonna stack a traditional linear solver at the end to give us the basis functions’ weights.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/darcy-flow/architecture.jpeg" alt="Darcy Flow Neural ODE Architecture" />
        <figcaption>Fig 1. Architecture</figcaption>
    </div>
    <br />
</figure>

<p><strong>How to Backpropagate?</strong></p>

<p>In order for our model to learn anything we’re going to need to pass the gradients backwards from the linear solver layer to the ResNet. In this dream world we could simply update our parameters like so:</p>

\[\theta^{l+1} = \theta^l - \alpha \cdot \frac{DL}{D\theta} \tag{4}\]

<p>But how do we know \(\frac{DL}{D\theta}\) if we have this weird linear solver layer in our system? i.e. what does it mean to calculate the gradient of the loss with respect to the parameters of the linear solver layer - even more so when we have how many N steps the solver will take to converge?</p>

<p>But this is weird … we have no way of knowing how many steps the</p>

<!-- **Optimality Condition** -->

<!-- \mathcal{F}(u)  -->

<!-- We're going to implmenet a differentiable optimization layer (I know, crazy) that will allow us to  -->

  </div><a class="u-url" href="/deep-learning/2024/03/11/darcy-flow.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/"></data>
  
    <div class="wrapper">
  
      <div class="footer-col-wrapper" style="margin-top: -20px;">
        <div class="footer-col">
        </div>
        <div class="footer-col">
          <p></p>
        </div>
      </div>
  
      <div class="social-links"><ul class="social-media-list"><li><a href="https://github.com/akshaytrikha"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">akshaytrikha</span></a></li><li><a href="https://www.linkedin.com/in/akshay-trikha"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">akshay-trikha</span></a></li></ul>
</div>
  
    </div>
</footer>
  </body>

</html>
