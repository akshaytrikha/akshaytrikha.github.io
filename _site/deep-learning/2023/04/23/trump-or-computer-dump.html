<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Trump or Computer Dump? | Akshay Trikha</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Trump or Computer Dump?" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html" />
<meta property="og:url" content="http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html" />
<meta property="og:site_name" content="Akshay Trikha" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-04-23T14:53:17-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Trump or Computer Dump?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-04-23T14:53:17-07:00","datePublished":"2023-04-23T14:53:17-07:00","headline":"Trump or Computer Dump?","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html"},"url":"http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Akshay Trikha" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Akshay Trikha</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Trump or Computer Dump?</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2023-04-23T14:53:17-07:00" itemprop="datePublished">Apr 23, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>

<gradio-app src="https://akshaytrikha-gpt2-trump.hf.space"></gradio-app>

<p><br />
In early 2019 OpenAI published some <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">findings</a> showing that large language models, when trained on collosal amounts of text, begin to behave in unpredictably intelligently on tasks they weren’t originally trained on. Humoursly, part the original dataset for the model was scraped from Reddit - which isn’t always known to be the home of constructive conversation and accurate information so it’s even more susprising how good their model was!</p>

<p>A year after that, while taking a natural language processing course I thought it would be fun to finetune GPT-2 on a corpus of then President Trump’s tweets seeing as he has such a disctinct style and voice. I obtained his tweets from the super convenient <a href="https://www.thetrumparchive.com/">Trump Twitter Archive</a> which contained ~56,500 of his tweets from 2009 until he was kicked off the platform on January 8th, 2021. The project took me around 3 weeks if I remember correctly and this weekend as I was dogsitting I remembered that project and got excited about trying to recreate it with the latest tools available, namely HuggingFace. It was awesome to have a benchmark to compare against to measure and truly appreciate the efficiency and ergonimcs of HuggingFace’s transformers library. I was able to replicate the finetuning thorugh a short notebook + host it using gradio on a HuggingFace space for you to play with in just a couple of days.</p>

  </div><a class="u-url" href="/deep-learning/2023/04/23/trump-or-computer-dump.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
    <data class="u-url" href="/"></data>
  
    <div class="wrapper">
  
      <div class="footer-col-wrapper" style="margin-top: -20px;">
        <div class="footer-col">
        </div>
        <div class="footer-col">
          <p></p>
        </div>
      </div>
  
      <div class="social-links"><ul class="social-media-list"><li><a href="https://github.com/akshaytrikha"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">akshaytrikha</span></a></li><li><a href="https://www.linkedin.com/in/akshay-trikha"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">akshay-trikha</span></a></li></ul>
</div>
  
    </div>
</footer>
  </body>

</html>
