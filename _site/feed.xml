<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-07-03T11:41:19-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Akshay Trikha</title><subtitle></subtitle><entry><title type="html">Trump or Computer Dump?</title><link href="http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html" rel="alternate" type="text/html" title="Trump or Computer Dump?" /><published>2023-04-23T16:53:17-05:00</published><updated>2023-04-23T16:53:17-05:00</updated><id>http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump</id><content type="html" xml:base="http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html"><![CDATA[<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>

<gradio-app src="https://akshaytrikha-gpt2-trump.hf.space"></gradio-app>

<p><br />
In early 2019 OpenAI published some <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">findings</a> showing that large language models, when trained on colossal amounts of text, begin to behave in unpredictably intelligently on tasks they weren’t originally trained on. Humorously, part the original dataset for the model was scraped from Reddit - which isn’t always known to be the home of constructive conversation and accurate information so it’s even more surprising how good their model was!</p>

<p>A year after that, while taking a natural language processing course I thought it would be fun to finetune GPT-2 on a corpus of then President Trump’s tweets seeing as he has such a distinct style and voice. I obtained his tweets from the super convenient <a href="https://www.thetrumparchive.com/">Trump Twitter Archive</a> which contained ~56,500 of his tweets from 2009 until he was kicked off the platform on January 8th, 2021. The project took me around 3 weeks if I remember correctly and this weekend as I was dog sitting I remembered that project and got excited about trying to recreate it with the latest tools available, namely HuggingFace. It was awesome to have a benchmark to compare against to measure and truly appreciate the efficiency and ergonomics of HuggingFace’s transformers library. I was able to replicate the finetuning through a short notebook + host it using gradio on a HuggingFace space for you to play with in just a couple of days.</p>]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Bias-Expressivity Trade-off</title><link href="http://localhost:4000/deep-learning/2019/11/09/bias-expressivity-tradeoff.html" rel="alternate" type="text/html" title="The Bias-Expressivity Trade-off" /><published>2019-11-09T15:53:17-06:00</published><updated>2019-11-09T15:53:17-06:00</updated><id>http://localhost:4000/deep-learning/2019/11/09/bias-expressivity-tradeoff</id><content type="html" xml:base="http://localhost:4000/deep-learning/2019/11/09/bias-expressivity-tradeoff.html"><![CDATA[]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Futility of Bias-Free Learning and Search</title><link href="http://localhost:4000/deep-learning/2019/07/25/futility-of-bias-free-learning-and-search.html" rel="alternate" type="text/html" title="The Futility of Bias-Free Learning and Search" /><published>2019-07-25T16:53:17-05:00</published><updated>2019-07-25T16:53:17-05:00</updated><id>http://localhost:4000/deep-learning/2019/07/25/futility-of-bias-free-learning-and-search</id><content type="html" xml:base="http://localhost:4000/deep-learning/2019/07/25/futility-of-bias-free-learning-and-search.html"><![CDATA[]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[]]></summary></entry></feed>