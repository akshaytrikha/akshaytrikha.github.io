<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-07T12:03:14-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Akshay Trikha</title><subtitle></subtitle><entry><title type="html">(Bitter) Lessons from Scaling Machine Learning Interatomic Potentials</title><link href="http://localhost:4000/research/2025/09/25/scaling.html" rel="alternate" type="text/html" title="(Bitter) Lessons from Scaling Machine Learning Interatomic Potentials" /><published>2025-09-25T06:11:17-07:00</published><updated>2025-09-25T06:11:17-07:00</updated><id>http://localhost:4000/research/2025/09/25/scaling</id><content type="html" xml:base="http://localhost:4000/research/2025/09/25/scaling.html"><![CDATA[<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<p><em>Or, Why My Master‚Äôs Thesis Failed</em></p>

<p><strong>TL;DR:</strong> I tested whether a plain transformer encoder (no equivariance, no periodic boundary conditions) can learn interatomic forces as data &amp; params scale vs. EquiformerV2. EqV2 followed clean power laws and hit much lower loss. My transformer failed and plateaued hard, leading me to believe my featurization was insufficient. I share the wins &amp; failures.</p>

<p>For EquiformerV2:</p>

\[\begin{aligned}
&amp;\quad\text{Parameter scaling law:} &amp;&amp;L \approx 7.76\times 10^{2}\, P^{-0.383} \\
&amp;\quad\text{Compute scaling law:} &amp;&amp;L \approx 4.99\times 10^{5}\, C^{-0.339} \\
&amp;\quad\text{Data scaling law:} &amp;&amp;L \approx 6.47\times 10^{1}\, D^{-0.242}
\end{aligned}\]

<p>For Transformer:</p>

\[\text{Power laws not clean enough}\]

<p>Code is open sourced <a href="https://github.com/akshaytrikha/materials-scaling">here</a>.</p>

<hr />
<p><br />
A little over a year ago I got pretty interested in training neural networks that can learn physics from data. Around the time machine learning interatomic potential (NNIP) models were becoming popular. These models broadly take as input a material‚Äôs atomic configuration and predict properties related to its potential energy. All the papers I was reading had great results, but I felt they were selling an incomplete story because they were missing scaling information. To me, understanding how a model scales is perhaps the most important factor and having just 1 datapoint of a final test loss was insufficient to understand how any architecture would stand the test of time.</p>

<p>I tried to investigate whether vanilla transformer encoders, given sufficient data, could learn to predict material properties as well as architectures explicitly equivariant architectures, like EquiformerV2 (EqV2) <a href="https://arxiv.org/pdf/2306.12059">[1]</a>. Inspired by the Bitter Lesson <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">[2]</a>, my hypothesis was that that transformers would scale more slowly than specialized architectures but that their scaling laws <a href="https://arxiv.org/pdf/2001.08361">[3]</a><a href="https://arxiv.org/pdf/1712.00409">[4]</a><a href="https://arxiv.org/pdf/2210.16859">[5]</a> would hold out over more orders-of-magnitude (OOM).</p>

<p>I failed to stably train transformers most likely due to the featurization I chose. I still learned a great deal along the way, and found power laws for EqV2.</p>

<p><br />
<strong>Why the math makes sense:</strong></p>

<p>A Transformer is a graph neural network (GNN) on a complete graph with learned edge weights <a href="https://arxiv.org/pdf/1704.01212">[6]</a>. A graph in a GNN is created through a rule, either a known relation between nodes i.e. this paper cites another or a cutoff i.e. this atom is too far from the other so we assume they won‚Äôt interact.</p>

<iframe id="attentionFrame" src="http://localhost:4000/assets/scaling/attention-graph.html" width="100%" style="border:0; border-radius:8px; box-shadow:0 2px 8px rgba(0,0,0,.1); background:#fff"></iframe>
<script>
  const f = document.getElementById('attentionFrame');
  f.addEventListener('load', () => {
    const doc = f.contentDocument || f.contentWindow.document;
    f.style.height = doc.documentElement.scrollHeight + 'px';
  });
</script>

<p><br />
A simple argument between the two architectures is that they fall into the bias - expressivity tradeoff. My take is that since self-attention on a fully connected graph is mathematically equivalent to message passing <a href="https://arxiv.org/pdf/1704.01212">[6]</a> it should be able to learn weights between atoms without having to describe them.</p>

<p><br />
<strong>Task and Dataset</strong></p>

<p>MLIPs are trained to take in a set of atoms and their positions to predict the structure‚Äôs energy and typically the forces on the atoms as well as the structure‚Äôs stresses.</p>

<p>A common criticism of NNIPs trained on Density Functional Theory (DFT) calculations is that those datasets are relaxed structures around 0K. This means that they‚Äôre not physically relevant in most cases to us because we don‚Äôt live around 0K.</p>

<p>The Open Materials 2024 (OMat24) <a href="https://huggingface.co/datasets/facebook/OMAT24">[7]</a> dataset is a 110 million crystal structure dataset that addresses this problem by focusing on including non-equilibrium (high temperature, structurally perturbed) samples. It‚Äôs also one of the largest datasets of its kind.</p>

<hr />

<p><br />
<strong>Sin #1: Transformer featurization</strong></p>

<p>In my attempt at training transformers I wanted to use as few inductive biases as possible. i.e. no equivariant features, no invariant features, no periodic boundary conditions. This was an attempt to learn everything from the data + augmentation, regardless of sample efficiency.</p>

<p>I used a standard embedding layer for atom types to give the model a dictionary lookup of what each atom is across different structures. This was important because the model needed to understand that each atom is the same in different structures but is modified by its context, similar to how each word is the same in different sequences and is modified by its context. The 3D positions were concatenated with the embedding vector because I thought the model might have an easier time disentangling meaning vs. saving parameters by adding the positions to the embeddings.</p>

<!-- $$
\begin{align}
&\text{Input feature matrix: } \left[
\begin{array}{ccc|ccc}
e_{11} & \cdots & e_{1d} & x_1 & y_1 & z_1 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
e_{N1} & \cdots & e_{Nd} & x_N & y_N & z_N
\end{array}
\right]
\in \mathbb{R}^{N\times(d+3)}
\end{align}
$$ -->

\[\begin{array}{c@{}c@{}c}
\text{Input feature matrix: } &amp;
\left[
\begin{array}{ccc|ccc}
e_{11} &amp; \cdots &amp; e_{1d} &amp; x_1 &amp; y_1 &amp; z_1 \\
\vdots &amp; \ddots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
e_{N1} &amp; \cdots &amp; e_{Nd} &amp; x_N &amp; y_N &amp; z_N
\end{array}
\right] &amp;
\in \mathbb{R}^{N\times(d+3)}
\\[-2ex]
&amp; \begin{array}{c@{\mkern8mu}c}
\underbrace{\hphantom{e_{11}\ \cdots\ e_{1d}}}_{\text{atomic embeddings}} &amp;
\underbrace{\hphantom{x_1\ y_1\ z_1}}_{\text{positional encoding}}
\end{array}
\end{array}\]

<p>This was purposefully not a rotationally invariant featurization as I wanted to see if the model could learn this through augmentation. It also did not account for the fact crystal structures are periodic, which means that forces on atoms can come from adjacent cells. My findings are that this featurization led to the model learning global structure energy and stresses well, but not 3D per-atom forces. This isn‚Äôt to say that forces weren‚Äôt learned at all, but they were certainly not comparable to EquiformerV2.</p>

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="http://localhost:4000/assets/scaling/train-force-loss.png" alt="transformer train force loss sweep" />
        <figcaption>Sweeping 1M parameter transformers on 100k structures</figcaption>
    </div>
    <br />
</figure>

<p>I got stuck with this phenomena of an apparent plateau in force loss only for certain hyperparam configs. There were other runs that would break through but still plateaued much higher than the EqV2. I suspect that training stability had a role with the former, and ultimately the lack of periodic boundary conditions impacting the latter.</p>

<p>Tough lessons learned:</p>
<ul>
  <li>The models just want to learn and given enough parameters the loss will go down even if they aren‚Äôt learning the right thing.</li>
  <li>If scaling behavior doesn‚Äôt appear in smaller OOMs it‚Äôs unlikely it will magically appear later</li>
</ul>

<p><br />
<strong>Sin #2: Not starting with individual experiments</strong></p>

<p>Instead, I rushed to create a more complex <a href="https://github.com/akshaytrikha/materials-scaling">set of scripts</a> that would automatically run scaling experiments over multiple OOMs. I came up with what I thought was a clever way of iterating through models:</p>

<!-- ```Python
class MetaTransformerModels:
    def __init__(
        self,
        vocab_size,
        max_seq_len,
    ):
        """Initializes TransformerModels with a list of configurations"""
        self.configurations = [
            {"d_model": 2, "depth": 1, "n_heads": 1, "d_ff_mult": 4}, # 2,280 params
            ...
            {"d_model": 256, "depth": 3, "n_heads": 2, "d_ff_mult": 4}, # 2,414,000 params
        ]
        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len

    def __getitem__(self, idx):
        """Retrieves transformer model corresponding to the configuration at idx"""
        config = self.configurations[idx]

        return XTransformerModel(
            num_tokens=self.vocab_size,
            d_model=config["d_model"],
            depth=config["depth"],
            n_heads=config["n_heads"],
            d_ff_mult=config["d_ff_mult"],
        )

    def __len__(self):
        return len(self.configurations)

    def __iter__(self):
        for idx in range(len(self.configurations)):
            yield self[idx]
``` -->

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="http://localhost:4000/assets/scaling/MetaTransformerModels.png" alt="MetaTransformerModels" />
    </div>
    <br />
</figure>

<p>On paper this sounds great to iterate over model sizes and lazily instantiate them, but in practice each OOM brings nuance and unexpectedness in behavior through e.g. hyperparameter sensitivity like early stoping. Instead, I should‚Äôve started with small manual overfitting experiments and gradually increasing the parameters and data <a href="http://karpathy.github.io/2019/04/25/recipe/">[8]</a>.</p>

<p><br />
<strong>Sin #3: Not starting with a small, in-memory, dataset</strong></p>

<p>Over all the experiments I ran I found that dataloading was typically the bottleneck in training time. This is an example of a bad dataset class I wrote. The devil is in the details because the <code class="language-plaintext highlighter-rouge">AseDBDataset.get_atoms(idx)</code> call looks like a simple getter but is actually doing disk I/O.</p>

<div style="display: grid; grid-template-columns: 1.2fr 1fr; gap: 20px; margin: 20px 0;">
<style>
  .code-comparison pre,
  .code-comparison code {
    font-size: 0.85em;
  }
</style>
<div class="code-comparison">

    <p><strong>Bad approach ‚ùå</strong></p>

    <p><img src="http://localhost:4000/assets/scaling/bad-approach.png" alt="Bad approach code" /></p>

    <p>The result is that all of this work repeats every call:</p>
    <ul>
      <li>With random shuffling causing worst-case random disk access</li>
      <li>Across multiple worker processes (each with their own DB connections)</li>
    </ul>

    <p>This is painful.</p>

  </div>
<div class="code-comparison">

    <p><strong>Better approach ‚úÖ</strong></p>

    <p><img src="http://localhost:4000/assets/scaling/better-approach.png" alt="Better approach code" /></p>

    <p>A very simple solution is to start experimenting with a very small dataset and iterate through it completely to cache it before training.</p>

  </div>
</div>

<p>At a small dataset scale the first approach didn‚Äôt matter. But it wasted a lot of time as I scaled the dataset size.</p>

<hr />

<p><br />
<strong>Win #1: Some EquiformerV2 results</strong></p>

<div class="eqv2-results-container">
  <div class="result-img">
    <img src="http://localhost:4000/assets/scaling/eqv2-param-scaling.png" alt="EquiformerV2 Parameter Scaling" />
  </div>
  <div class="result-img">
    <img src="http://localhost:4000/assets/scaling/eqv2-compute-scaling.png" alt="EquiformerV2 Compute Scaling" />
  </div>
  <div class="result-img">
    <img src="http://localhost:4000/assets/scaling/eqv2-dataset-scaling.png" alt="EquiformerV2 Dataset Scaling" />
  </div>
</div>

<style>
.eqv2-results-container{
  /* full width breakout */
  width: 100vw;
  max-width: 100vw;
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);

  /* 3-up grid */
  padding-inline: 2rem;           /* optional gutter */
  box-sizing: border-box;
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 2rem;
  align-items: stretch;
}

.result-img{ 
  width:100%; 
  margin:0;
  display: flex;
  align-items: center;
  justify-content: center;
}

.result-img img{
  width: 100%;
  height: 350px;
  object-fit: contain;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

/* responsive wrap-downs */
@media (max-width: 1200px){ .eqv2-results-container{ grid-template-columns: repeat(2, 1fr); } }
@media (max-width: 768px){  .eqv2-results-container{ grid-template-columns: 1fr; } }
</style>

<p><br />
Takeaways:</p>
<ul>
  <li>Demonstrated power laws</li>
  <li>Data scaling shows diminishing returns compared to param scaling</li>
  <li>Identified compute-optimal training configs with a real pareto frontier</li>
  <li>Data and training pipeline worked for EqV2, so there was something inherently wrong with the transformer</li>
</ul>

<p><br />
<strong>Win #2: Making an inference visualization tool</strong></p>

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="http://localhost:4000/assets/scaling/inference.gif" alt="inference visualization" />
    </div>
    <br />
</figure>

<p>This ended up being a very useful debugging tool to understand how the models were behaving. For example as a sanity check it was nice to see the transformer wasn‚Äôt only predicting 0 forces nor the mean of the dataset, and also that models were learning smaller magnitude forces as well as larger ones.</p>

<p><br />
<strong>Win #3: Making a scaling law experiment run tool</strong></p>

<div class="scaling-plots-container">
  <div id="eqv2-scaling-plot1" class="plot-main"></div>
  <div class="plot-dual-container">
    <div id="eqv2-scaling-plot2" class="plot-secondary"></div>
    <div id="eqv2-scaling-plot3" class="plot-secondary"></div>
  </div>
</div>

<style>
.scaling-plots-container{
  /* full width breakout */
  width: 100vw;
  max-width: 100vw;
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);

  /* keep your 3-up grid */
  padding-inline: 2rem;           /* optional gutter */
  box-sizing: border-box;
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 2rem;
  align-items: stretch;
}

/* keep these from Option A */
.plot-dual-container{ display: contents; }
.plot-main, .plot-secondary{ width:100%; height:400px; margin:0; }

/* responsive wrap-downs */
@media (max-width: 1200px){ .scaling-plots-container{ grid-template-columns: repeat(2, 1fr); } }
@media (max-width: 768px){  .scaling-plots-container{ grid-template-columns: 1fr; } }
</style>

<!-- Plotly -->
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>

<!-- Load your module and render -->
<script type="module">
  import { renderEqV2Scaling } from "/assets/scaling/eqv2-scaling.js";
  renderEqV2Scaling('eqv2-scaling-plot1', 'eqv2-scaling-plot2', 'eqv2-scaling-plot3');
</script>

<p>This tool helped group families of runs and studying their behavior on the same plot. Before training any models I established naive (no deep learning) baselines that would help understand the loss number. The three baselines were:</p>
<ul>
  <li>Loss while predicting 0 everywhere</li>
  <li>Loss while predicting the mean of the dataset everywhere</li>
  <li>Loss from a k = 1 nearest-neighbor Markov model</li>
</ul>

<p><br />
<strong>Win #4: Getting to talk to John Jumper about scaling AlphaFold</strong></p>

<p>I think my life peaked a little when I got to have a pizza and a beer with John Jumper at the YC AI Startup School and pitch him this unfinished research.</p>

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="http://localhost:4000/assets/scaling/john-jumper.jpg" alt="meeting John Jumper" />
        <figcaption>I met one of my heroes and he turned out to be extremely kind.</figcaption>
    </div>
    <br />
</figure>

<!-- A Dummy‚Äôs Guide to Empirically Deriving Scaling Laws

(& What not to do)

1. Source compute first (assuming you already have a large dataset)
2. Do NOT start by YOLOing a huge run
    - Each run is only 1 data point so a large one is still only 1 data point
    - Stay small as long as possible
3. Your first goal is to overfit a model. Do this by following http://karpathy.github.io/2019/04/25/recipe/
4. Don‚Äôt forget to visualize & verify your outputs
5. Training efficiency bells & whistles
    1. FlashAttention, mixed precision gradient clipping,
-->

<h4 id="references">References:</h4>

<ul>
  <li>[1] <a href="https://arxiv.org/pdf/2306.12059">EquiformerV2</a> - Liao et al., 2023</li>
  <li>[2] <a href="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">The Bitter Lesson</a> - Richard Sutton, 2019</li>
  <li>[3] <a href="https://arxiv.org/pdf/2001.08361">Scaling Laws for Neural Language Models</a> - Kaplan et al., 2020</li>
  <li>[4] <a href="https://arxiv.org/pdf/1712.00409">Deep Learning Scaling Is Predictable, Empirically</a> - Hestness et al., 2017</li>
  <li>[5] <a href="https://arxiv.org/pdf/2210.16859">A Solvable Model of Neural Scaling Laws</a> - Maloney et al., 2022</li>
  <li>[6] <a href="https://arxiv.org/pdf/1704.01212">Neural Message Passing for Quantum Chemistry</a> - Gilmer et al., 2017</li>
  <li>[7] <a href="https://huggingface.co/datasets/facebook/OMAT24">Open Materials 2024 (OMat24) Dataset</a> - Meta AI, 2024</li>
  <li>[8] <a href="http://karpathy.github.io/2019/04/25/recipe/">A Recipe for Training Neural Networks</a> - Andrej Karpathy, 2019</li>
  <li><a href="https://arxiv.org/pdf/2203.15556">Training Compute-Optimal Large Language Models (Chinchilla)</a> - Hoffmann et al., 2022</li>
  <li><a href="https://distill.pub/2020/circuits/equivariance/">Naturally Occurring Equivariance in Neural Networks</a> - Olah et al., 2020</li>
</ul>]]></content><author><name></name></author><category term="Research" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">AI Rubber Ducky Pair Programmer</title><link href="http://localhost:4000/product/2025/05/23/ducky.html" rel="alternate" type="text/html" title="AI Rubber Ducky Pair Programmer" /><published>2025-05-23T09:06:17-07:00</published><updated>2025-05-23T09:06:17-07:00</updated><id>http://localhost:4000/product/2025/05/23/ducky</id><content type="html" xml:base="http://localhost:4000/product/2025/05/23/ducky.html"><![CDATA[<p>Please try out the extension on the VSCode extension marketplace and let me know what you think! Enter your email to get access:</p>

<!-- gated signup + link reveal -->
<div id="signup-container">
  <form id="signup-form">
    <input type="email" id="email-input" name="email" placeholder="you@domain.com" required="" />
    <button type="submit">Sign up ‚Üí</button>
  </form>
</div>

<div id="link-container" style="display:none; margin-top:1em;">
  <a href="https://marketplace.visualstudio.com/items?itemName=duckydev.duckydev" target="_blank" rel="noopener">
    üê§ Get the Ducky VSCode extension
  </a>
</div>

<script src="https://cdn.jsdelivr.net/npm/@supabase/supabase-js"></script>

<script>
  // initialize Supabase client
  const supabaseClient = supabase.createClient(
    'https://jpwoombwzqxfxebrpzkl.supabase.co',
    'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Impwd29vbWJ3enF4ZnhlYnJwemtsIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDgyMTY4NDgsImV4cCI6MjA2Mzc5Mjg0OH0.UzRsuRw63TN6rFNtLtBpXZy8JKvrzH1tliS7D_SgI50'
  );

  const form            = document.getElementById('signup-form');
  const emailInput      = document.getElementById('email-input');
  const signupContainer = document.getElementById('signup-container');
  const linkContainer   = document.getElementById('link-container');

  // if user already signed up, show the link immediately
  if (localStorage.getItem('duckySignedUp') === 'true') {
    signupContainer.style.display = 'none';
    linkContainer.style.display   = 'block';
  }

  form.addEventListener('submit', async (evt) => {
    evt.preventDefault();
    const email = emailInput.value.trim();
    if (email) {
        // insert email into your Supabase table named "emails"
        const { data, error } = await supabaseClient
        .from('Users')
        .insert([{ email }]);

        if (error) {
        console.error(error);
        alert('Oops‚Äîsomething went wrong. Please try again.');
        } else {
        localStorage.setItem('duckySignedUp', 'true');
        signupContainer.style.display = 'none';
        linkContainer.style.display   = 'block';
        }
    }
  });
</script>

<!-- end gated block -->
<p><br /></p>

<hr />

<p><br />
A few weeks ago I thought I was bothering my coworker by calling him to help debug something too often. In two separate conversations I exclaimed because I knew exactly what my bug was without him having said a word on the call‚Äîand felt embarrassed as he laughed at me through the computer screen.</p>

<p>I thought: <em>what if I could create an AI rubber ducky that I could call instead of him?</em></p>

<p><br />
<strong>UX</strong></p>

<p>In my head I wanted to create a UX where I could talk to the rubber ducky and it would both hear me and see the code I was talking about, just like a Zoom call with your coworker.</p>

<div class="video-container">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/g_rt8n8Dnec?rel=0" frameborder="0" allowfullscreen="">
  </iframe>
</div>

<p>You can either click on a single line, or highlight multiple lines to send them as context to the model.</p>

<p><br />
<strong>Quickstart</strong></p>

<ol>
  <li>
    <p>BYO-API key</p>

    <figure>
     <div style="text-align: center;">
         <img src="http://localhost:4000/assets/ducky/set-api-key.png" alt="set API key" />
     </div>
     <br />
 </figure>
  </li>
  <li>
    <p>Start Call</p>

    <figure>
     <div style="text-align: center;">
         <img src="http://localhost:4000/assets/ducky/start-call.gif" alt="start call" />
     </div>
     <br />
 </figure>
  </li>
  <li>
    <p>Show Conversation History</p>

    <figure>
     <div style="text-align: center;">
         <img src="http://localhost:4000/assets/ducky/show-conversation-history.gif" alt="show conversation history" />
     </div>
     <br />
 </figure>
  </li>
</ol>

<p><br />
<strong>User Test</strong></p>

<p>To test if this is useful I created a dummy codebase for training a simple Vision Transformer for classification on a small subset of the <a href="https://huggingface.co/datasets/uoft-cs/cifar100">CIFAR-100</a> dataset. I introduced a small bug in the <code class="language-plaintext highlighter-rouge">init()</code> of the model:</p>

<div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code>‚ùå num_patches = (img_size // patch_size) * (img_size // patch_size - 1)
‚úÖ num_patches = (img_size // patch_size) * (img_size // patch_size)
</code></pre></div></div>

<p>and asked my friend to use the AI rubber ducky to help him debug what was going wrong.</p>

<div class="video-container">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/ITSSergQAos?rel=0" frameborder="0" allowfullscreen="">
  </iframe>
</div>

<p><br />
<strong>Why now?</strong></p>

<p>I‚Äôm seeing a divergence in the way people write code at Berkeley vs. at my job. Students / entrepreneurs are vibe coding to the max while my coworkers / friends at other companies are using LLM tools but still handwriting their bugfixes &amp; features. I believe this is because production codebases are:</p>
<ol>
  <li>are about more than just the code i.e. there are business decisions being made outside the repo</li>
  <li>are too large &amp; expensive to feed into a prompt</li>
  <li>will always need to be debugged, whether the code is human or AI generated</li>
</ol>

<p>I can see a future where there are fewer programmers than there are today, but I believe the paradigm of asking an AI for help getting unstuck before bothering your coworker is here to stay for all knowledge work.</p>

<p><br />
<strong>Cost</strong></p>

<p>This entire project cost around ~$40 to make, split evenly between a Cursor subscription and the OpenAI Realtime API. The spike in cost is while using the full 4o model while my friend did a 10 min user test. 4o-mini is around 4x <a href="https://openai.com/api/pricing/">cheaper</a> for audio tokens and 10x cheaper for text tokens.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/ducky/api-cost.png" alt="openai realtime api cost" />
    </div>
    <br />
</figure>

<p><br />
<strong>Future features:</strong></p>
<ul>
  <li>Integrate with Cursor / GitHub Copilot</li>
  <li>Transcribe user‚Äôs voice into chat</li>
  <li>Allow users to modify system prompt for personality</li>
  <li>Ducky learns from past conversations</li>
  <li>Track user file edits</li>
  <li>Let ducky have a cursor</li>
  <li>Visualize debugging attempt paths e.g.</li>
</ul>

<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            background-color: #f9fafb;
            padding: 20px;
        }

        .container {
            display: flex;
            flex-direction: column;
            margin: 0 auto;
            max-width: 768px;
            width: 100%;
            background-color: #f9fafb;
            padding: 16px;
            border-radius: 8px;
            gap: 40px;
        }

        .header {
            text-align: center;
        }

        .title {
            font-size: 24px;
            <!-- font-weight: bold; -->
            margin-bottom: 8px;
            color: #111827;
        }

        .subtitle {
            color: #6b7280;
            margin-top: -20px;
        }

        .timeline-container {
            position: relative;
        }

        .timeline-line {
            position: absolute;
            left: 84px;
            top: 0;
            bottom: 0;
            width: 4px;
            background-color: #60a5fa;
        }

        .timeline-events {
            display: flex;
            flex-direction: column;
            gap: 32px;
        }

        .timeline-event {
            display: flex;
            align-items: flex-start;
            position: relative;
        }

        .timestamp {
            width: 60px;
            padding-top: 8px;
            padding-right: 12px;
            text-align: right;
            font-weight: 600;
            color: #6b7280;
            flex-shrink: 0;
        }

        .content-box {
            flex-grow: 1;
            background-color: white;
            border-radius: 8px;
            padding: 16px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
            border-left: 4px solid;
        }

        .border-red { border-left-color: #ef4444; }
        .border-yellow { border-left-color: #eab308; }
        .border-blue { border-left-color: #3b82f6; }
        .border-purple { border-left-color: #a855f7; }
        .border-green { border-left-color: #22c55e; }

        .event-title {
            margin-bottom: 8px;
        }

        .title-red { color: #ef4444; }
        .title-yellow { color: #eab308; }
        .title-blue { color: #3b82f6; }
        .title-purple { color: #a855f7; }
        .title-green { color: #22c55e; }

        .event-description {
            color: #374151;
            line-height: 1.5;
        }

        .code-block {
            background-color: #f3f4f6;
            padding: 8px;
            margin-top: 8px;
            border-radius: 4px;
            font-family: 'Courier New', Consolas, Monaco, monospace;
            font-size: 14px;
            color: #1f2937;
        }

        .success-message {
            margin-top: 8px;
            font-weight: 600;
            color: #059669;
        }

        @media (max-width: 768px) {
            .container {
                width: 95%;
            }
            
            .timestamp {
                width: 50px;
                font-size: 14px;
                padding-right: 8px;
            }
            
            .timeline-line {
                left: 66px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h4 style="margin-bottom: -10px">Vision Transformer Debugging Journey</h4>
        </div>
        <!-- Timeline visualization -->
        <div class="timeline-container">
            <!-- Main timeline line -->
            <div class="timeline-line"></div>
            <!-- Timeline events -->
            <div class="timeline-events">
                <!-- Initial problem -->
                <div class="timeline-event">
                    <div class="timestamp">0:00</div>
                    <div class="content-box border-red">
                        <h4 class="event-title">Problem Identified</h4>
                        <p class="event-description">User is experiencing an error with a Vision Transformer (ViT) model implementation</p>
                    </div>
                </div>
                <!-- Code exploration -->
                <div class="timeline-event">
                    <div class="timestamp">0:30</div>
                    <div class="content-box border-yellow">
                        <h4 class="event-title">Initial Exploration</h4>
                        <p class="event-description">Started exploring model.py, looking at model initialization parameters and forward pass implementation</p>
                        <div class="code-block">model = ViT(
                            img_size=IMG_SIZE,
                            patch_size=16,
                            emb_dim=64,
                            depth=4,
                            num_heads=2
                            )
                        </div>
                    </div>
                </div>
                <!-- First insight -->
                <div class="timeline-event">
                    <div class="timestamp">2:00</div>
                    <div class="content-box border-blue">
                        <h4 class="event-title">First Insight</h4>
                        <p class="event-description">Discovered potential issue with patch calculation:</p>
                        <div class="code-block">num_patches = (img_size // patch_size) * (img_size // patch_size - 1) # Wrong calculation</div>
                    </div>
                </div>
                <!-- Key discovery -->
                <div class="timeline-event">
                    <div class="timestamp">5:00</div>
                    <div class="content-box border-purple">
                        <h4 class="event-title">Key Discovery</h4>
                        <p class="event-description">Identified critical error in forward pass:</p>
                        <div class="code-block">cls_tokens = self.cls_token.expand(b, -1, -1)</div>
                        <p class="event-description" style="margin-top: 8px;">Variable 'cls_tokens' is referenced but 'self.cls_token' is undefined in the model!</p>
                    </div>
                </div>
                <!-- Solution -->
                <div class="timeline-event">
                    <div class="timestamp">7:30</div>
                    <div class="content-box border-green">
                        <h4 class="event-title">Solution</h4>
                        <p class="event-description">Added missing class token initialization in __init__:</p>
                        <div class="code-block">self.cls_token = nn.Parameter(torch.randn(1, 1, emb_dim))</div>
                        <p class="success-message">‚úì Bug fixed! Model now works properly</p>
                    </div>
                </div>
            </div>
        </div>
    </div>
</body>
</html>

<h4 id="references">References:</h4>

<ul>
  <li><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/realtime-audio-reference">Azure / OpenAI Realtime API docs</a></li>
  <li><a href="https://platform.openai.com/docs/guides/realtime">OpenAI Realtime API docs</a></li>
  <li><a href="https://github.com/openai/openai-realtime-console">OpenAI Realtime API Example Repo</a></li>
  <li><a href="https://openai.com/api/pricing/">OpenAI API Pricing</a></li>
  <li><a href="https://code.visualstudio.com/api/working-with-extensions/publishing-extension">Publishing VSCode Extensions</a></li>
</ul>]]></content><author><name></name></author><category term="Product" /><summary type="html"><![CDATA[Please try out the extension on the VSCode extension marketplace and let me know what you think! Enter your email to get access:]]></summary></entry><entry><title type="html">Physics-Constrained Neural Networks for Solving Partial Differential Equations</title><link href="http://localhost:4000/how%20to/2024/03/11/darcy-flow.html" rel="alternate" type="text/html" title="Physics-Constrained Neural Networks for Solving Partial Differential Equations" /><published>2024-03-11T06:11:17-07:00</published><updated>2024-03-11T06:11:17-07:00</updated><id>http://localhost:4000/how%20to/2024/03/11/darcy-flow</id><content type="html" xml:base="http://localhost:4000/how%20to/2024/03/11/darcy-flow.html"><![CDATA[<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

<p><strong>Background</strong></p>

<p>A neural network learns a function to map inputs to outputs. Because solutions to differential equations are also functions we can use neural networks to solve them!</p>

<p>Oftentimes in physics we want solutions to differential equations that obey the laws of physics. For example, we don‚Äôt want solutions that violate conservation laws or predict things like negative mass or density. How do we force a neural network to only predict functions that satisfy some physical constraint?</p>

<hr />
<p><br />
<strong>Darcy Flow</strong></p>

<p>Let‚Äôs take the example of 2D darcy flow, a time-independent partial differential equation (PDE) that describes the flow of a fluid through a porous medium. The solution to this PDE tells us the pressure distribution of the fluid at a given spatial point - and from that we can derive the flow rate and direction of fluid motion.</p>

<p>Darcy flow states:</p>

\[\begin{align}
    -\nabla \cdot q &amp;= f(x,y) &amp;&amp; x,y \in (0,1) \tag{1} \newline
    q &amp;= \nu(x,y) \nabla u(x,y) &amp;&amp; \tag{2} \newline
\end{align}\]

<p>With boundary condition</p>

\[\begin{align}
    \underbrace{u(x,y)}_{\text{PDE solution}} &amp;= 0 &amp;&amp; \qquad \qquad \quad x,y \in \partial(0,1)^2 \tag{3}
\end{align}\]

<p>Where:
\begin{align}
    u(x, y) &amp;= \text{pressure distribution}, \newline
    q &amp;= \text{flow rate}, \newline
    \nu(x, y) &amp;= \text{diffusion coefficient}, \newline
    \nabla &amp;= \text{divergence operator}, \newline
    f(x, y) &amp;= 
    \begin{cases} 
        \text{source} &amp; \text{if } f(x, y) &gt; 0 \newline
        \text{sink} &amp; \text{if } f(x, y) &lt; 0
    \end{cases}
    = 1
\end{align}</p>

<p>We can boil this fancy law down to a conservation of mass. In english, eq. \((1)\) states that the amount of fluid entering a region (\(-\nabla \cdot q\)) must either leave it or be accounted for by the source / sink (\(f(x,y)\)). For this problem, we‚Äôre setting \(f(x,y) = 1\) as a <em>hard constraint</em> to mean that there is a constant source of fluid in the system and we want to learn a model that predicts the pressure distribution. We‚Äôre going to train a model to solve the PDE given the hard, physics, constraint.</p>

<hr />
<p><br />
<strong>Data</strong></p>

<p>The dataset we‚Äôre using gives us the diffusion coefficient \(\nu(x, y)\) as input and has the pressure distribution \(u(x,y)\) as output labels. The model is then learning the relationship of how different materials‚Äô diffusion coefficients affect how fluids flow through them. In order to discretize the problem we‚Äôre also going to pass in a finite mesh so that our model has a spatial box to operate on. The mesh grid stays the same for every sample, but the diffusion coefficient changes. So the shapes of these are:</p>

<ol>
  <li>Mesh: [Batch, \(N_x\), \(N_y\), 2] where \(N_x\) &amp; \(N_y\) are the number of grid points and 2 represents the x,y coordinates at each point</li>
  <li>Diffusion coefficient \(\nu\): [Batch, \(N_x\), \(N_y\), 1] the same as above except we only have 1 dimension to predict for each grid point</li>
</ol>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/darcy-flow/architecture_1.png" alt="Darcy Flow Neural ODE Architecture" />
        <figcaption>Fig 1. Architecture</figcaption>
    </div>
    <br />
</figure>

<!-- # TODO: talk about loss -->

<p>Data is available courtesy of the <a href="http://tensorlab.cms.caltech.edu/users/anima/">Anima AI + Science Lab</a> Lab at CalTech as part of their work <a href="https://arxiv.org/pdf/2010.08895">[3]</a>. For training data we will use the url <a href="https://drive.google.com/file/d/16kmLCmuPe_q_wpphEXtgJswFtxHE6Plf/view?usp=sharing">[4]</a>  and for validation the url <a href="https://drive.google.com/file/d/1sQBpormuajXlf2h3YiMnL9qVzzFoSwBB/view?usp=sharing">[5]</a>.</p>

<hr />
<p><br />
<strong>Constrained Model</strong></p>

<p>Ok great, we have a partial differential equation and our goal is to train a neural network that guesses solutions to this. In our heads, we imagine our solutions could be expressed as a linear combination of basis functions and their corresponding weights. This is a fancy way of saying that our final function might be comprised of applying combinations of differently sized lego-brick functions.</p>

\[u = \sum_i K_i w_i \tag{4}\]

<p>Where:
\begin{align}
    K &amp;= \text{set of basis functions}, \\<br />
    w &amp;= \text{weights for each basis function} \\<br />
\end{align}</p>

<p>Now we see a problem - we need to optimize two things with this approach: generating optimal basis functions in addition to optimal weights for our final prediction to be good. This type of problem is called bi-level optimization and we‚Äôre gonna solve it with wizardry (or at least I think so).</p>

<p>So now our plan is to have the neural network predict the basis functions, and then we‚Äôll stack a linear solver to calculate the weights. The solver will ensure they satisfy the constraint imposed by the forcing function \(f(x,y)=1\). In other words, <strong>the linear solver layer is how we implement our physical constraint.</strong></p>

<!-- \begin{align}
    \tag{5}
\end{align} -->

<p>\begin{align}
    K(\theta) w^{\ast}(\theta) = f(x,y) \iff K(\theta) w^{\ast}(\theta) - 1 = 0 &amp; \tag{5} \newline
\end{align}</p>

<p>(using \(w^*\) to represent the optimal weights for a given set of basis functions \(K\)).</p>

<hr />
<p><br />
<strong>How to Backpropagate?</strong></p>

<p>In order for our model to learn anything we‚Äôre going to need to pass the gradients backwards from the linear solver layer to the ResNet. In an ideal world we could simply write out a chain rule like so:</p>

<!-- $$\theta^{l+1} = \theta^l - \alpha \cdot \frac{DL}{D\theta} \tag{6}$$ -->

\[\frac{\partial \mathcal{L}}{\partial \theta} = \frac{\partial \mathcal{L}}{\partial w^{*}} \cdot \frac{\partial w^{*}}{\partial \theta} \tag{6}\]

<p>But with this weird linear solver layer in our system what is the relationship between between its weights and the neural network‚Äôs weights? Is it computationally feasible to directly calculate \(\frac{\partial \mathcal{L}}{\partial w^{*}}\)? To do so would mean we would have to find the gradients of each step of the solver with respect to the parameters, for every iteration it takes to converge. This quickly becomes a nightmare in terms of computation, memory, and stability.</p>

<p>Instead of doing that we could also link \(w^*(\theta)\) and \(\theta\) through the optimality condition we defined in eq. \((5)\). However, because the linear solver‚Äôs weights \(w\) don‚Äôt have an explicit dependence on \(\theta\), we‚Äôll have to differentiate the expression implicitly <a href="https://arxiv.org/pdf/2105.15183">[1]</a>:</p>

\[\begin{align}
    \frac{\partial}{\partial \theta} \left( K(\theta) w^{\ast}(\theta) - 1 \right) = 0 \newline

   \underbrace{\frac{\partial F}{\partial w}}_{A} \underbrace{\frac{\partial w^*(\theta)}{\partial \theta}}_{J_1} + \underbrace{\frac{\partial F}{\partial K}}_{B} \frac{\partial K(\theta)}{\partial \theta} = 0 &amp; \tag{7} \newline
\end{align}\]

<p>We can directly compute \(A\) and \(B\) as outputs of the forward pass and our weights:</p>

\[\begin{align}
    A &amp;= \frac{\partial}{\partial w} \left( K(\theta) w - 1 \right) = K(\theta), \newline
    B &amp;= \frac{\partial}{\partial K} \left( K w(\theta) - 1 \right) = w^*(\theta), \newline
    J_1 &amp;= \frac{\partial w^*(\theta)}{\partial \theta} = \quad ?
\end{align}\]

<p>But what is \(J_1\)? Let‚Äôs rearrange our expression to see if we can solve for it.</p>

\[\begin{align}
    AJ_1 + B \frac{\partial K(\theta)}{\partial \theta} = 0 \newline
    J_1 = A^{-1} B \frac{\partial K(\theta)}{\partial \theta} \newline
    \frac{\partial w^*(\theta)}{\partial \theta} = -K(\theta)^{-1} w^*(\theta) \frac{\partial K}{\partial \theta} \tag{8}
\end{align}\]

<p>Great! So now we can compute all the terms that lead to \(\frac{\partial w^*}{\partial \theta}\). Let‚Äôs substitute our expression in eq. \((8)\) into eq. \((6)\):</p>

\[\frac{\partial \mathcal{L}}{\partial \theta} = - \frac{\partial \mathcal{L}}{\partial w^{*}} \cdot K(\theta)^{-1} w^*(\theta) \frac{\partial K}{\partial \theta} \tag{9}\]

<p>We now have a way to backpropagate through the linear solver üéâ</p>

<hr />
<p><br />
<strong>Implementation</strong></p>

<p>Our model class is going to look something like this:</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/darcy-flow/ConstrainedModel.png" alt="ConstrainedModel class implementation" />
        <figcaption>Fig 2. ConstrainedModel class</figcaption>
    </div>
    <br />
</figure>

<!-- ```Python
class ConstrainedModel(nn.Module):
    def __init__(self, in_dim: int, hidden_dims: Tuple[int], n_basis_functions: int):
        super().__init__()
        self.n_basis_functions = n_basis_functions
        self.resnet = ResNet(in_dim, hidden_dims, n_basis_functions)

    def forward(
            self, mesh: torch.Tensor, diffusion_coeffs: torch.Tensor
        ) -> torch.Tensor:
        """
        Args:
            mesh: mesh coordinates (batch x Nx x Ny x 2) 
            diffusion_coeffs: diffusion coefficients (batch x Nx x Ny x 1)
        Returns:
            u: solution (batch x Nx x Ny x 1)
        """
        # 1. neural network predicts basis functions 
        basis = self.resnet(mesh, diffusion_coeffs)
        
        # 2. solve linear system Kw = b for optimal basis functions' weights
        K, b = self.setup_linear_system(basis, mesh, diffusion_coeffs)
        w = LinearSolve.apply(K, b)
        
        # 3. compute solution u = basis * weights
        u = basis @ w.T
        
        return u
``` -->

<p>But in order to pass gradients through the <code class="language-plaintext highlighter-rouge">LinearSolve</code> layer we‚Äôre going to have to tell PyTorch our new backward pass rule:</p>

<!-- ```Python
import torch

class LinearSolve(torch.autograd.Function):
    @staticmethod
    def forward(ctx, K, b):
        """Solve the linear system Kw = b. K is a batch of matrices and b is a batch of vectors.
        K has to be square (invertible).

        Args:
            ctx (torch.autograd.function.LinearSolveBackward): autograd context
            K (torch.Tensor): matrix K (K x N x N)
            b: torch.Tensor - vector b (K x N)

        Returns:
            (torch.Tensor): solution (K x N)
        """
        # solve the linear system
        w_star = torch.linalg.solve(K, b)

        # save for backward pass
        ctx.save_for_backward(K, w_star, b)

        return w_star
    
    @staticmethod
    def backward(ctx, upstream_grad):
        """Compute the gradient of the linear solver with respect to K and b.

        Args:
            ctx: autograd context
            upstream_grad (torch.Tensor): upstream gradient from the loss
        Returns:
            torch.Tensor (B x N x N), torch.Tensor (B x N): gradients with respect to K and b
        """
        # Goal: Use implicit differentiation to compute dw*/dŒ∏ = -inv(K) * w* * dK/dŒ∏

        # 1. retrieve useful tensors from forward pass
        K, b, w_star = ctx.saved_tensors

        # 2. compute vjp_grad = -K^{-T} * upstream_grad
        vjp_grad = torch.linalg.solve(-torch.transpose(K, 1, 2), upstream_grad)

        # 3. define the optimality condition: Kw* - b = 0
        def optimality_cond(_K, _b):
            """In our case _b is always 1, but here we've left it general"""
            return torch.bmm(_K, w_star.unsqueeze(-1)).squeeze(-1) - _b

        # 4. perform vector-Jacobian product (VJP) for optimality condition
        evaluations, funct_ = torch.autograd.functional.vjp(optimality_cond, K, b)

        # 5. apply VJP function to compute final gradients
        return funct_(vjp_grad)
``` -->

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/darcy-flow/LinearSolve.png" alt="LinearSolve class implementation" />
        <figcaption>Fig 3. LinearSolve class</figcaption>
    </div>
    <br />
</figure>

<p>Ok so there‚Äôs a lot going on here so let‚Äôs break it down. For the implementation we rewrite eq. \((9)\) like so:</p>

\[\frac{\partial \mathcal{L}}{\partial \theta} = w^*(\theta) \cdot \underbrace{v \cdot -K(\theta)^{-1}}_{\texttt{vjp_grad}} \underbrace{\frac{\partial K}{\partial \theta}}_{J_2} \tag{10}\]

<p>Where</p>

\[\begin{align}
    v &amp;= \frac{\partial \mathcal{L}}{\partial w^{*}} = \text{upstream gradient}, \newline
    J_2 &amp;= \text{Jacobian of the function $K$ with respect to $\theta$} \newline
\end{align}\]

<p>\(\texttt{vjp_grad} \times J_2\) is called a vector-Jacobian product (VJP) and it‚Äôs the reason why reverse-mode auto differentiation is so efficient [6]. Because taking the inverse of a matrix in \(K^{-1}\) is sometimes not numerically stable we instead find \(\texttt{vjp_grad}\) by solving the linear system:</p>

\[\begin{align}
    K^Tx = - \frac{\partial \mathcal{L}}{\partial w^{*}} \tag{11} \newline
\end{align}\]

<p>Then we formulate a functional form of the VJP and finally apply it to <code class="language-plaintext highlighter-rouge">vjp_grad</code>.</p>

<h4 id="references">References:</h4>

<ul>
  <li><a href="https://arxiv.org/pdf/2105.15183">[1] Efficient and Modular Implicit Differentiation</a></li>
  <li><a href="https://arxiv.org/pdf/2207.08675">[2] Learning Differentiable Solvers For Systems With Hard Constraints</a></li>
  <li><a href="https://arxiv.org/pdf/2010.08895">[3] Fourier Neural Operator for Parametric Partial Differential Equations</a></li>
  <li><a href="https://drive.google.com/file/d/16kmLCmuPe_q_wpphEXtgJswFtxHE6Plf/view?usp=sharing">[4] Darcy Flow Training Data</a></li>
  <li><a href="https://drive.google.com/file/d/1sQBpormuajXlf2h3YiMnL9qVzzFoSwBB/view?usp=sharing">[5] Darcy Flow Validation Data</a></li>
  <li>[6] To jog the memory, the Jacobian is the matrix of all partial derivatives for a function.</li>
</ul>

\[\begin{align}
    \text{Jacobian}(f) = 
    \begin{bmatrix}
        \frac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \newline
        \vdots &amp; \ddots &amp; \vdots \newline
        \frac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n}
    \end{bmatrix}
\end{align}\]

<p>But if we have a large neural network calculating the entire Jacobian matrix becomes too computationally expensive. Instead we take advantage of its structure by only multiplying the upstream grad \(v\) with every row of \(J\) without ever having to instantiate \(J\) in its entirety.</p>]]></content><author><name></name></author><category term="How To" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Universal Mask Finding</title><link href="http://localhost:4000/product/2023/09/24/universal-mask-finding.html" rel="alternate" type="text/html" title="Universal Mask Finding" /><published>2023-09-24T13:29:00-07:00</published><updated>2023-09-24T13:29:00-07:00</updated><id>http://localhost:4000/product/2023/09/24/universal-mask-finding</id><content type="html" xml:base="http://localhost:4000/product/2023/09/24/universal-mask-finding.html"><![CDATA[<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/fast_mask_finding.png" alt="ML defect detection" />
        <figcaption>Fig 1. Mask finding of a chocolate bar in the making</figcaption>
    </div>
    <br />
</figure>

<p><br />
<strong>Preface</strong></p>

<p>I write using analogy of working for Willy Wonka‚Äôs chocolate factory to demonstrate that this method could be used in any manufacturing context that uses computer vision for defect detection. It‚Äôs also just fun to think about mass producing chocolate.</p>

<p><br />
<strong>Background</strong></p>

<p>You recently got hired as an Oompa Loompa software engineer working at Willy Wonka‚Äôs chocolate factory on a team that images chocolate accross the various stages of the manufacturing line in order to find defects. They use all sorts of fancy cameras: 2D, 3D, radiographs - they know their stuff. This approach lets them both find defects quicker than waiting till the end of the factory line to find out something‚Äôs bad, and also allows scrapping defective material much earlier in the manufacturing process to cut losses.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/defect_detection.gif" alt="ML defect detection" />
        <figcaption>Fig 2. Defect Detection</figcaption>
    </div>
    <br />
</figure>

<p>However, before we try and find defects in an image of a bar we‚Äôve got to first find an area to search through for defects ‚Äì otherwise a defect model might get confused and flag the background as defective. We‚Äôd also want constrain the defect search space as much as possible to make the defect model more efficient at flagging defects. I refer to this specific semantic segmentation task as ‚Äúmask finding‚Äù.</p>

<p>For example, if your human neural network saw images in Fig 2. of bars-in-the-making, what features would you think distinguish it from the background? Maybe that a bar has four corners, is a different color than the background, and has a different surface consistency. If we as huamns can distinguish chocolate from it‚Äôs background we could probably write a rules based program for finding a segmentation mask, but the types of input images often rapidly changes:</p>

<ul>
  <li>Oompa Loompa scientists may iterate on the product and future chocolates might have different dimensions</li>
  <li>imaging conditions of inspection stations may change (different light color, angle)</li>
  <li>we may want to use a different type of camera (laser profiler, darkfield black boxes)</li>
</ul>

<p>All of which leads to a dizzying combination of image possibilities and you‚Äôd be playing a game of cat-and-mouse to keep up by writing separate image processing pipelines for each variation.</p>

<!-- It would be maybe 10x more difficult to build and maintain 40 different traditional image processing pipelines with human coded logic to find masks compared to using one ML model. -->

<p><br />
<strong>ML</strong></p>

<p>So then how do we scale this process up for hundreds of thousands of images a month across multiple image and camera types? Universal Mask Finding is an ML model I trained while I was at the chocolate factory to solve this problem. The magic of deep learning is that we don‚Äôt explicitly program the model to find the features we just mentioned, we ask the model to hopefully learn them automatically across our dataset.</p>

<p>The model is based on an off-the-shelf <a href="https://paperswithcode.com/method/u-net#:~:text=U%2DNet%20is%20an%20architecture,architecture%20of%20a%20convolutional%20network">U-Net</a> segmentation architecture with pretrained resnet34 weights and infers a pixel-level segmentation mask. That means, for every pixel in an image the model predicts the probability that it belongs to the foreground, or background. I fine-tuned a pretrained model originally trained on the ImageNet task with the relevant images of chocolates. More about transfer learning <a href="/how%20to/2023/07/04/transfer-learning.html">here</a>.</p>

<p>We measure the performance of a segmentation model based on the Intersection over Union score, or IOU, which is a fancy way of saying what % of the prediction overlapped with a ground truth label.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/IOU.gif" alt="mIOU" />
        <figcaption>Fig 2. Measuring performance with IOU</figcaption>
    </div>
    <br />
</figure>

<p>We can also do interesting things here with the inference probabilities like setting a lower <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" target="\_blank">threshold</a> for the mask class like 0.3 to increase the recall (but at the cost of decreased precision).</p>

<p><br />
<strong>Generalization</strong></p>

<p>Often in machine learning we want a model to be able to generalize on a given task as much as possbile, and that‚Äôs probably a good marker that it‚Äôs learning something about the underlying nature of the problem. Somewhat counterintuitively, I found that I get the best performance by allowing a single model to learn features across all the varying ‚Äúimage types‚Äù. Examples of things I would separate into different ‚Äúimage type‚Äù categories include different cameras, backgrounds, and components.</p>

<p>To prove the model generalizes well I ran a simple study where I gradually increased the number of image types in the training set, and measured performance against unseen image types.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/generalization.png" alt="generalization" />
        <figcaption>Fig 3. Generalization</figcaption>
    </div>
    <br />
</figure>

<p>And if we include all image types (dotted lines) during training we can close the gap to virtually perfect. This is particularly nice for two reasons:</p>

<ol>
  <li>because there is a lot of uncertainty in incoming images in production</li>
  <li>because this approach saves Willy Wonka $$$ per month in server costs compared to deploying 40 different models for each image type</li>
</ol>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/image_types.gif" alt="image-types" />
        <figcaption>Fig 4. Different image types encountered in production</figcaption>
    </div>
    <br />
</figure>

<p><br />
<strong>Building Trust: Classification Reviewer Model</strong></p>

<p>Some fellow Oompa Loompa cocoa scientists asked me how they could trust my mask finding model‚Äôs inferences - a very fair question. I tried developing rule based metrics to see if a mask had 4 corners, straight lines etc. but it turned out to be a difficult task as no one criteria would capture the complexities of judging mask finding quality. I realized that a deep learning model classifier would outperform any handwritten metrics I could come up with. This new classifier would run after each segmentation prediction and would have 3 classes:</p>

<figure style="display: flex; gap: 4.9%; text-align: center;">
    <div style="width: 30%;">
        <img src="http://localhost:4000/assets/mask-finding/grade_1.png" alt="ML defect detection" />
        <figcaption>Grade 1: inferred mask is perfect</figcaption>
    </div>
    <div style="width: 30%;">
        <img src="http://localhost:4000/assets/mask-finding/grade_2.png" alt="ML defect detection" />
        <figcaption>Grade 2: inferred mask is imperfect but can still be used for downstream defect detection</figcaption>
    </div>
    <div style="width: 30%;">
        <img src="http://localhost:4000/assets/mask-finding/grade_3.png" alt="ML defect detection" />
        <figcaption>Grade 3: inferred mask has poor quality and can't be trusted for defect defection</figcaption>
    </div>
</figure>

<p>Because I already had ground truth human labels for the segmentation task I already had a great set of examples for Grade 1. All I needed to do was find a handful Grade 2 and 3 failures and to my surprise the classifier had an <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">AUC</a> of 0.995! We could now use the classifier‚Äôs predictions to understand whether or not to trust the segmentation model.</p>

<p><br />
<strong>Building Trust: Historical Performance</strong></p>

<p>To further build trust in the model I monitor how each image type, which is demarked by a metadata tag in the dataset, performs over time, making sure that all their respective mIOUs are close to 1. I also periodically check my historical data log to inspect how the model is doing in production and shuffle the dataset with the latest examples of failures. It‚Äôs very difficult for me to perfectly monitor this model in production and so I also rely on my fellow Oompa Loompa scientists and production managers to notify me if they notice a drop in inference quality.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/historical_performance.png" alt="historical-performance" />
        <figcaption>Fig 5. Tracking historical performance </figcaption>
    </div>
    <br />
</figure>

<p>In general the number of image types for the model to run on have only been growing so I typicially also evaluate a new model based on a previous iteration‚Äôs dataset just to double check that I‚Äôm not noticing a drop in quality for older image types. It‚Äôs also fun to evaluate an older iteration on the latest model‚Äôs test dataset to quantify the improvement.</p>

<p>I‚Äôm proud this ML system has been robust while integrating new cameras and chocolate types and was able to scale to help the chocolate factory grow while delivering the best chocolate possible.</p>

<!-- ![Fast Mask Finding](../../../assets/fast_mask_finding.gif) -->]]></content><author><name></name></author><category term="Product" /><summary type="html"><![CDATA[Fig 1. Mask finding of a chocolate bar in the making]]></summary></entry><entry><title type="html">Transfer Learning in Computer Vision</title><link href="http://localhost:4000/how%20to/2023/07/04/transfer-learning.html" rel="alternate" type="text/html" title="Transfer Learning in Computer Vision" /><published>2023-07-04T09:06:17-07:00</published><updated>2023-07-04T09:06:17-07:00</updated><id>http://localhost:4000/how%20to/2023/07/04/transfer-learning</id><content type="html" xml:base="http://localhost:4000/how%20to/2023/07/04/transfer-learning.html"><![CDATA[<p>Recall when beginning to train a neural network its weights might be initialized randomly. What if you could start training with a leg up because the network already has some useful information inside of it? Transfer learning <em>is</em> that leg up where you can repurpose models trained on similar tasks and use them for your specific task instead of training from scratch.</p>

<p>Benefits include:</p>

<ul>
  <li>training faster</li>
  <li>less training data</li>
  <li>boosts generalization</li>
  <li>might improve accuracy</li>
</ul>

<figure>
    <br />
    <img src="http://localhost:4000/assets/transfer-learning/transfer-learning.png" alt="transfer learning diagram" />
    <figcaption style="text-align: center">diagram inspired by Stanford CS 329P 2021 slides</figcaption>
    <br />
</figure>

<p>Above we see an example of using ImageNet pre-trained weights to classify objects from the Fashion MNIST dataset. A bit of an overkill example, but you get the point.</p>

<p>There are three main ways you might go about transfer learning:</p>

<ol>
  <li>
    <p>Use the pre-trained model as a pre-trained feature extractor by freezing its hidden layers (and replacing its head with your own). This works well when your task isn‚Äôt too different from the pre-trained model‚Äôs task.</p>
  </li>
  <li>
    <p>Finetune the pre-trained model completely by not freezing any layers</p>
  </li>
  <li>
    <p>Finetune but freeze some number layers. There are two main risks with this approach: (1) the higher-level early neurons are more specialized than we expected, and (2) splitting &amp; combining two arbitrary layers causes a mismatch in learned features that cannot be relearned by earlier layers.</p>
  </li>
</ol>

<p>Thanks to the open-source community there are many places you can find pre-trained models, like PyTorch Hub, HuggingFace, and TensorFlow Hub to name a few.</p>

<h3 id="segmentation-example">Segmentation Example</h3>

<h4 id="1-loading-the-model">1. Loading the Model</h4>

<p>I‚Äôve used the DeepLabv3 model from <a href="https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/">Pytorch Hub</a> and gotten some pretty great results. We first start by loading the pre-trained weights into the model:</p>

<pre><code class="language-Python">import torchvision

# there are three model sizes: MobileNet, ResNet50, and ResNet101
weights = torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT
model = torchvision.models.segmentation.deeplabv3_resnet50(weights=weights).to(device)
</code></pre>

<p>Then we need to replace the classifier layer but to do that we need to know its input dimension. The easiest way I‚Äôve found to do that is to use <code class="language-plaintext highlighter-rouge">torchinfo.summary()</code>:</p>

<pre><code class="language-Python">from torchinfo import summary

summary(
    model=model,
    input_size=(2, 3, 1024, 1024),
    col_names=["input_size", "output_size", "num_params", "trainable"],
    col_width=20,
    row_settings=["var_names"]
)
</code></pre>

<p>which prints out:</p>

<pre><code class="language-#1">===========================================================================================================================
Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable
===========================================================================================================================
DeepLabV3 (DeepLabV3)                              [2, 3, 1024, 1024]   [2, 2, 1024, 1024]   --                   True
‚îú‚îÄIntermediateLayerGetter (backbone)               [2, 3, 1024, 1024]   [2, 2048, 128, 128]  --                   True
‚îÇ    ‚îî‚îÄConv2d (conv1)                              [2, 3, 1024, 1024]   [2, 64, 512, 512]    9,408                True
‚îÇ    ‚îî‚îÄBatchNorm2d (bn1)                           [2, 64, 512, 512]    [2, 64, 512, 512]    128                  True
‚îÇ    ‚îî‚îÄReLU (relu)                                 [2, 64, 512, 512]    [2, 64, 512, 512]    --                   --
‚îÇ    ‚îî‚îÄMaxPool2d (maxpool)                         [2, 64, 512, 512]    [2, 64, 256, 256]    --                   --
‚îÇ    ‚îî‚îÄSequential (layer1)                         [2, 64, 256, 256]    [2, 256, 256, 256]   --                   True
‚îÇ    ‚îÇ    ‚îî‚îÄBottleneck (0)                         [2, 64, 256, 256]    [2, 256, 256, 256]   75,008               True
‚îÇ    ‚îÇ    ‚îî‚îÄBottleneck (1)                         [2, 256, 256, 256]   [2, 256, 256, 256]   70,400               True
‚îÇ    ‚îÇ    ‚îî‚îÄBottleneck (2)                         [2, 256, 256, 256]   [2, 256, 256, 256]   70,400               True
.                                                           .                                                       .
.                                                           .                                                       .
.                                                           .                                                       .
‚îú‚îÄDeepLabHead (classifier)                         [2, 2048, 128, 128]  [2, 2, 128, 128]     --                   True
‚îÇ    ‚îî‚îÄASPP (0)                                    [2, 2048, 128, 128]  [2, 256, 128, 128]   --                   True
‚îÇ    ‚îÇ    ‚îî‚îÄModuleList (convs)                     --                   --                   15,206,912           True
‚îÇ    ‚îÇ    ‚îî‚îÄSequential (project)                   [2, 1280, 128, 128]  [2, 256, 128, 128]   328,192              True
‚îÇ    ‚îî‚îÄConv2d (1)                                  [2, 256, 128, 128]   [2, 256, 128, 128]   589,824              True
‚îÇ    ‚îî‚îÄBatchNorm2d (2)                             [2, 256, 128, 128]   [2, 256, 128, 128]   512                  True
‚îÇ    ‚îî‚îÄReLU (3)                                    [2, 256, 128, 128]   [2, 256, 128, 128]   --                   --
‚îÇ    ‚îî‚îÄConv2d (4)                                  [2, 256, 128, 128]   [2, 2, 128, 128]     514                  True
===========================================================================================================================
Total params: 39,633,986
Trainable params: 39,633,986
Non-trainable params: 0
Total mult-adds (T): 1.31
===========================================================================================================================
Input size (MB): 25.17
Forward/backward pass size (MB): 17650.16
Params size (MB): 158.54
Estimated Total Size (MB): 17833.87
===========================================================================================================================
</code></pre>

<p>From the <code class="language-plaintext highlighter-rouge">DeepLabHead (classifier)</code> line which has shape <code class="language-plaintext highlighter-rouge">[batch, in_channels, height, width]</code> we can see that we need <code class="language-plaintext highlighter-rouge">in_channels=2048</code> for our new classifier.</p>

<pre><code class="language-Python"># modify classifier layer for desired number of classes
# number for in_channels was found by examining the model architecture
model.classifier = DeepLabHead(in_channels=2048, num_classes=NUM_CLASSES)
</code></pre>

<h4 id="2-tensor-shapes">2. Tensor Shapes</h4>

<p>Then let‚Äôs try and understand the model‚Äôs output. From its documentation: <em>‚ÄúThe model returns an</em> <code class="language-plaintext highlighter-rouge">OrderedDict</code> <em>with two Tensors that are of the same height and width as the input Tensor ‚Ä¶</em> <code class="language-plaintext highlighter-rouge">output['out']</code> <em>contains the semantic masks, and</em> <code class="language-plaintext highlighter-rouge">output['aux']</code> <em>contains the auxiliary loss values per-pixel.‚Äù</em></p>

<pre><code class="language-Python"># output['out'] is what we really want
# output is a tensor with shape [batch_size, num_classes, height, width]
output = model(X_train.to(device))["out"]
</code></pre>

<p>In order to calculate the loss we have to massage the tensors a bit. I chose <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss()</code> as the loss function and its documentation allows the input to have shape <code class="language-plaintext highlighter-rouge">[batch_size, num_classes, height, width]</code> but its target must have shape <code class="language-plaintext highlighter-rouge">[batch_size, height, width]</code> so with help from the <code class="language-plaintext highlighter-rouge">einops</code> package:</p>

<pre><code class="language-Python">from einops import rearrange

# rearrange target
target = rearrange(target, "bat cla height width -&gt; (bat cla) height width")

# calculate loss
loss = loss_fn(output, target)
</code></pre>

<p>Now we notice that for each batch there is a dimension in the first index of the tensor for <code class="language-plaintext highlighter-rouge">logit(false)</code> and <code class="language-plaintext highlighter-rouge">logit(true)</code> which is redundant. We can just keep <code class="language-plaintext highlighter-rouge">logit(true)</code> for each batch, take that softmax, binarize the predictions, and finally calculate accuracy.</p>

<pre><code class="language-Python"># modify output to be in format [logit(true)] for each sample
output = output[:, 1, :, :]

# take softmax
output = nn.functional.softmax(output, dim=1)

# binarize predictions &amp; calculate accuracy
y_pred = (output &gt; 0.5).type(torch.int32)
accuracy_fn = torchmetrics.Accuracy(task="binary", num_classes=NUM_CLASSES)
accuracy = accuracy_fn(y_pred)
</code></pre>

<h4 id="3-wrapping-up">3. Wrapping Up</h4>

<p>With the loss and accuracy for a batch we can go ahead and train our model. The only other trouble I had with with my custom dataset was that I didn‚Äôt realize <code class="language-plaintext highlighter-rouge">torchvision.transforms.ToTensor()</code> automatically divides a tensor by 256 so make sure your data is correctly scaled!</p>

<p>For experiment tracking I used <a href="https://wandb.ai/site">wandb</a> which was super simple to set up and let me easily visualize how tweaking some hyperparameters like batch size affected this model‚Äôs training. You can find how I created the dataset class, training loops, and experiment tracking for segmentation transfer learning in my <a href="https://github.com/akshaytrikha/transfer-learning/blob/main/segmentation/scripts/">GitHub</a> repository.</p>

<h3 id="classification-example">Classification Example</h3>

<p>Transfer learning for a classification task is virtually the same as segmentation and slightly easier. We start again by loading the pre-trained weights into the model and replacing the classifier layer so that it has the right input and output dimensions for our target problem.</p>

<pre><code class="language-Python">import torchvision

# there are multiple other model sizes
weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
model = torchvision.models.efficientnet_b0(weights=weights).to(device)

# if you wanted to freeze base layers
for param in model.features.parameters():
    param.requires_grad = False

# modify classifier layer for number of classes
# added dropout for more robustness
model.classifier = nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True),
    nn.Linear(in_features=CLASSIFIER_IN_FEATURES, out_features=NUM_CLASSES),
).to(device)
</code></pre>

<p>The only tricky thing here is calculating <code class="language-plaintext highlighter-rouge">CLASSIFIER_IN_FEATURES</code>, which was found by printing out the model‚Äôs layers using <code class="language-plaintext highlighter-rouge">torchinfo.summary()</code>. The training step is more straightforward than the segmentation example:</p>

<pre><code class="language-Python"># forward pass
# outputs are in the format [logit(true)] for each sample
# logit = log(unnormalized probability)
outputs = model(X_train.to(device))

# calculate loss &amp; accuracy
loss = loss_fn(outputs, y_train)
accuracy = accuracy_fn(outputs, y_train)
</code></pre>

<p>There you have it, now you can use transfer learning for both segmentation and classification tasks.</p>

<h4 id="references">References:</h4>

<ul>
  <li><a href="https://cs231n.github.io/transfer-learning">https://cs231n.github.io/transfer-learning</a></li>
  <li><a href="https://arxiv.org/pdf/1411.1792.pdf">How transferable are features in deep neural networks?</a></li>
  <li><a href="https://www.learnpytorch.io/06_pytorch_transfer_learning/">https://www.learnpytorch.io/06_pytorch_transfer_learning/</a></li>
</ul>]]></content><author><name></name></author><category term="How To" /><summary type="html"><![CDATA[Recall when beginning to train a neural network its weights might be initialized randomly. What if you could start training with a leg up because the network already has some useful information inside of it? Transfer learning is that leg up where you can repurpose models trained on similar tasks and use them for your specific task instead of training from scratch.]]></summary></entry><entry><title type="html">Trump or Computer Dump?</title><link href="http://localhost:4000/how%20to/2023/04/23/trump-or-computer-dump.html" rel="alternate" type="text/html" title="Trump or Computer Dump?" /><published>2023-04-23T14:53:17-07:00</published><updated>2023-04-23T14:53:17-07:00</updated><id>http://localhost:4000/how%20to/2023/04/23/trump-or-computer-dump</id><content type="html" xml:base="http://localhost:4000/how%20to/2023/04/23/trump-or-computer-dump.html"><![CDATA[<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.39.0/gradio.js"></script>

<gradio-app src="https://akshaytrikha-gpt2-trump.hf.space"></gradio-app>

<p>In early 2019 OpenAI published some <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">research</a> showing that large language models, when trained on colossal amounts of text, begin to behave unpredictably intelligently on tasks they weren‚Äôt originally trained on. Humorously, part of the original dataset for the model was scraped from Reddit - which isn‚Äôt always known to be the home of constructive conversation and accurate information so it‚Äôs even more surprising how good their model was!</p>

<p>In 2020, while taking an NLP, course I thought it would be fun to finetune GPT-2 on a corpus of then President Trump‚Äôs tweets seeing as he has such a distinct style and voice. I obtained his tweets from the super convenient <a href="https://www.thetrumparchive.com/">Trump Twitter Archive</a> which contained ~56,500 of his tweets from 2009 until he was banned from the platform on January 8th, 2021.</p>

<p>The original project took me around 3 weeks if I remember correctly and this weekend as I was dog sitting I remembered that project and got excited about trying to recreate it with the latest tools available, namely HuggingFace. It was awesome to have a benchmark to compare against to measure and truly appreciate the efficiency and ergonomics of HuggingFace‚Äôs transformers library. I was able to replicate the finetuning through a short <a href="https://github.com/akshaytrikha/GPT-2-Trump/blob/master/huggingface_trump.ipynb">notebook</a> + host it using gradio on a HuggingFace space in just a couple of days.</p>

<h3 id="finetuning-with-huggingface-transformers">Finetuning with HuggingFace Transformers</h3>

<p>First I defined some hyperparams for training:</p>

<pre><code class="language-Python"># define some params for model
batch_size = 8
epochs = 15
learning_rate = 5e-4
epsilon = 1e-8
warmup_steps = 1e2
sample_every = 100  # produce sample output every 100 steps
max_length = 140  # max length used in generate() method of model
</code></pre>

<p>Assuming the twitter data is downloaded and preprocessed, we need to tokenize the tweets (which means assign a unique <code class="language-plaintext highlighter-rouge">int</code> to every word or character) and load them into a dataloader. The <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> is really nice because it uses the right tokenizer for any specific model available on HuggingFace if you give it a model name e.g.<code class="language-plaintext highlighter-rouge">"gpt2-medium"</code>.</p>

<pre><code class="language-Python">from transformers import (
	AutoTokenizer
    TextDataset,
    DataCollatorForLanguageModeling
)

# create tokenized datasets
tokenizer = AutoTokenizer.from_pretrained(
    "gpt2-medium",
    pad_token='&lt;|endoftext|&gt;'
)

# custom load_dataset function because there are no labels
def load_dataset(train_path, dev_path, tokenizer):
    block_size = 128
    # block_size = tokenizer.model_max_length

    train_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=train_path,
          block_size=block_size)

    dev_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=dev_path,
          block_size=block_size)

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False,
    )
    return train_dataset, dev_dataset, data_collator

train_dataset, dev_dataset, data_collator = load_dataset(
	train_path, dev_path, tokenizer
)
</code></pre>

<p>Now we can instantiate the model, optimizer, and learning rate scheduler. I‚Äôd never experimented with dynamically changing the learning rate during training so using <code class="language-plaintext highlighter-rouge">get_linear_schedule_with_warmup()</code> was exciting. The resulting learning rate vs. epochs graph looked like this:</p>

<figure>
    <br />
	<div style="text-align: center">
		<img src="http://localhost:4000/assets/trumpdump/learning-rate.png" alt="linear learning rate schedule" style="width: 90%" />
	</div>
    <br />
</figure>

<p>which shows a delay in reaching the <code class="language-plaintext highlighter-rouge">learning_rate</code> by <code class="language-plaintext highlighter-rouge">num_epoch_steps</code> and then a slow decrease until the nth epoch.</p>

<pre><code class="language-Python">from transformers import (
    AutoModelWithLMHead,
    get_linear_schedule_with_warmup,
)

# AutoModelWithLMHead will pick GPT-2 weights from name
model = AutoModelWithLMHead.from_pretrained(
	model_name,
	cache_dir=Path('cache').resolve()
)

# necessary because of additional bos, eos, pad tokens to embeddings
model.resize_token_embeddings(len(tokenizer))

# create optimizer and learning rate schedule
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=epsilon)

training_steps = len(train_dataset) * epochs

# adjust learning rate during training
scheduler = get_linear_schedule_with_warmup(
	optimizer,
	num_warmup_steps = warmup_steps,
	num_training_steps = training_steps
)
</code></pre>

<p>Finally we can instantiate a <code class="language-plaintext highlighter-rouge">TrainingArguments</code> object which holds hyperparams and then run training from the <code class="language-plaintext highlighter-rouge">Trainer</code> object. This also prints out a pretty printed loss vs. step table.</p>

<pre><code class="language-Python">from transformers import (
    Trainer,
    TrainingArguments,
)

training_args = TrainingArguments(
    output_dir="./gpt-2-medium-trump",
    num_train_epochs=epochs,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    eval_steps = 400, # n update steps between two evaluations
    save_steps=800, # n steps per model save
    warmup_steps=500, # n warmup steps for learning rate scheduler
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
)

# train &amp; save model run after done
trainer.train()
trainer.save_model()
</code></pre>

<h3 id="inference-with-huggingface-transformers">Inference with HuggingFace Transformers</h3>

<p>Inference was even easier than training as all you need to do is set up a <code class="language-plaintext highlighter-rouge">pipeline</code> for the model:</p>

<pre><code class="language-Python">from transformers import pipeline

trump = pipeline(
	"text-generation",
	model="./gpt-2-medium-trump",
	tokenizer=tokenizer,
	config={"max_length":max_length}  # n tokens
)
</code></pre>

<p>Trump at your fingertips</p>

<pre><code class="language-Python">In:  trump("Today I'll be")[0]["generated_text"]

Out: "Today I'll be rallying w/ @FEMA, First Responders, Law Enforcement,
and First Responders of Puerto Rico to help those most affected by the
#IrmaFlood.https://t.co/gsFSghkmdM"
</code></pre>

<h4 id="resources">Resources:</h4>

<ul>
  <li><a href="https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt">HuggingFace Trainer</a></li>
  <li><a href="https://huggingface.co/gpt2">GPT-2 Model Card</a></li>
  <li><a href="https://github.com/akshaytrikha/GPT-2-Trump/blob/master/huggingface_trump.ipynb">GPT-2 Trump notebook</a></li>
</ul>]]></content><author><name></name></author><category term="How To" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Neural Style Transfer Webcam</title><link href="http://localhost:4000/product/2021/07/16/neural-style-transfer-webcam.html" rel="alternate" type="text/html" title="Neural Style Transfer Webcam" /><published>2021-07-16T05:50:00-07:00</published><updated>2021-07-16T05:50:00-07:00</updated><id>http://localhost:4000/product/2021/07/16/neural-style-transfer-webcam</id><content type="html" xml:base="http://localhost:4000/product/2021/07/16/neural-style-transfer-webcam.html"><![CDATA[<h3 id="overview">Overview</h3>

<p>I wanted to learn how to build and deploy an ML system in the real world and thought this would be a fun place to start. You can find my project‚Äôs code <a href="https://github.com/akshaytrikha/style-transfer">here</a>.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/style-transfer/style-transfer.gif" alt="style transfer demo gif" style="width: 100%" />
        <figcaption style="text-align: center">
            Try the demo yourself <a href="https://akshaytrikha.github.io/style-transfer/" target="_blank" rel="noopener noreferrer">here</a>
        </figcaption>
    </div>
    <br />
</figure>

<!-- <figure>
    <br>

        <img src="http://localhost:4000/assets/mask-finding/fast_mask_finding.png" alt="ML defect detection"/>
        <figcaption>Fig 1. Mask Finding</figcaption>

    <br>
</figure> -->

<p>Using your webcam as input, this project generates stylized images in 400ms intervals. It uses two pretrained Tensorflow.js neural networks, sourced from Reiichiro Nakano‚Äôs <a href="https://github.com/reiinakano/arbitrary-image-stylization-tfjs">arbitrary-image-stylization-tfjs</a> repo.</p>

<ol>
  <li>The first network is used to learn the style of a given image and generate a style representation.</li>
  <li>The second network is then used for style transfer, or using the style representation to generate a stylized output image.</li>
</ol>

<p>The original models were actually regular tf models, but Reiichiro distilled them into smaller networks so they would run faster. For a more detailed breakdown of how the networks work, check out his blog <a href="https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/">post</a>.</p>

<h3 id="building-the-react-app">Building the React App</h3>

<p>I used Nicholas Renotte‚Äôs <a href="https://github.com/nicknochnack/ReactComputerVisionTemplate">ReactComputerVisonTemplate</a> repo as a template for interfacing with a webcam and rendering output back on screen. The big idea is to use the <code class="language-plaintext highlighter-rouge">react-webcam</code> component to take screenshots, generate their style representations, and then display their stylized images. In pseudo-ish code the entire app is basically:</p>

<pre><code class="language-JavaScript">import Webcam from "react-webcam";

const predict = async () =&gt; {
    // First wait for models to load
    await loadModels();
    // init style image and generate style representation
    await initStyleImage();

    // every 400ms
    setInterval(() =&gt; {
        captureScreenshot();
        generateStylizedImage();
    }, 400);
}
</code></pre>

<h3 id="lessons-learned">Lessons learned</h3>

<ol>
  <li>
    <p>A very fun fact about using TensorFlow.js is that your browser is locally running inference, and all your data is kept on your device.</p>
  </li>
  <li>
    <p>You have to do a lot of things asynchronously e.g. fetching the models, loading the models, running inference. Timing all of this was how I decided to introduce the delay of 400ms.</p>
  </li>
  <li>
    <p>tfjs is slightly different to using tf in python in small ways. You have to use methods like <code class="language-plaintext highlighter-rouge">tf.ready().then(() =&gt; {})</code> that returns a promise when tfjs is ready (if the WebGL backend is ready).</p>
  </li>
  <li>
    <p>I used the html <code class="language-plaintext highlighter-rouge">canvas</code> element to display images and would use <code class="language-plaintext highlighter-rouge">document.getElementById("element-name").src = ...</code> directly to update an element. I standardized the display sizes in px to make it easier to deal with.</p>
  </li>
  <li>
    <p>To make them persistent I stored the model weights in the repo itself so that they‚Äôre accessible with e.g. <code class="language-plaintext highlighter-rouge">transferModel = await tf.loadGraphModel(process.env.PUBLIC_URL + '/models/style-prediction/model.json')</code></p>
  </li>
</ol>

<h4 id="resources">Resources:</h4>

<ul>
  <li><a href="https://github.com/reiinakano/arbitrary-image-stylization-tfjs">https://github.com/reiinakano/arbitrary-image-stylization-tfjs</a></li>
  <li><a href="https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/">https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/</a></li>
  <li><a href="https://styletransfer.art">https://styletransfer.art</a></li>
</ul>]]></content><author><name></name></author><category term="Product" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Barium Titanate Permittivity &amp;amp; Image Processing</title><link href="http://localhost:4000/research/2021/07/14/bto-image-processing.html" rel="alternate" type="text/html" title="Barium Titanate Permittivity &amp;amp; Image Processing" /><published>2021-07-14T14:53:17-07:00</published><updated>2021-07-14T14:53:17-07:00</updated><id>http://localhost:4000/research/2021/07/14/bto-image-processing</id><content type="html" xml:base="http://localhost:4000/research/2021/07/14/bto-image-processing.html"><![CDATA[]]></content><author><name></name></author><category term="Research" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Bias-Expressivity Trade-off</title><link href="http://localhost:4000/research/2019/11/09/bias-expressivity-tradeoff.html" rel="alternate" type="text/html" title="The Bias-Expressivity Trade-off" /><published>2019-11-09T13:53:17-08:00</published><updated>2019-11-09T13:53:17-08:00</updated><id>http://localhost:4000/research/2019/11/09/bias-expressivity-tradeoff</id><content type="html" xml:base="http://localhost:4000/research/2019/11/09/bias-expressivity-tradeoff.html"><![CDATA[]]></content><author><name></name></author><category term="Research" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Futility of Bias-Free Learning and Search</title><link href="http://localhost:4000/research/2019/07/25/futility-of-bias-free-learning-and-search.html" rel="alternate" type="text/html" title="The Futility of Bias-Free Learning and Search" /><published>2019-07-25T14:53:17-07:00</published><updated>2019-07-25T14:53:17-07:00</updated><id>http://localhost:4000/research/2019/07/25/futility-of-bias-free-learning-and-search</id><content type="html" xml:base="http://localhost:4000/research/2019/07/25/futility-of-bias-free-learning-and-search.html"><![CDATA[]]></content><author><name></name></author><category term="Research" /><summary type="html"><![CDATA[]]></summary></entry></feed>