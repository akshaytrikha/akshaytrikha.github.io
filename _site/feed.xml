<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-04-23T21:55:03-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Akshay Trikha</title><subtitle></subtitle><entry><title type="html">Trump or Computer Dump?</title><link href="http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html" rel="alternate" type="text/html" title="Trump or Computer Dump?" /><published>2023-04-23T14:53:17-07:00</published><updated>2023-04-23T14:53:17-07:00</updated><id>http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump</id><content type="html" xml:base="http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html"><![CDATA[<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>

<gradio-app src="https://akshaytrikha-gpt2-trump.hf.space"></gradio-app>

<p><br />
In early 2019 OpenAI published some <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">findings</a> showing that large language models, when trained on collosal amounts of text, begin to behave in unpredictably intelligently on tasks they weren’t originally trained on. Humoursly, part the original dataset for the model was scraped from Reddit - which isn’t always known to be the home of constructive conversation and accurate information so it’s even more susprising how good their model was!</p>

<p>A year after that, while taking a natural language processing course I thought it would be fun to finetune GPT-2 on a corpus of then President Trump’s tweets seeing as he has such a disctinct style and voice. I obtained his tweets from the super convenient <a href="https://www.thetrumparchive.com/">Trump Twitter Archive</a> which contained ~56,500 of his tweets from 2009 until he was kicked off the platform on January 8th, 2021. The project took me around 3 weeks if I remember correctly and this weekend as I was dogsitting I remembered that project and got excited about trying to recreate it with the latest tools available, namely HuggingFace. It was awesome to have a benchmark to compare against to measure and truly appreciate the efficiency and ergonimcs of HuggingFace’s transformers library. I was able to replicate the finetuning thorugh a short notebook + host it using gradio on a HuggingFace space for you to play with in just a couple of days.</p>]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[]]></summary></entry></feed>