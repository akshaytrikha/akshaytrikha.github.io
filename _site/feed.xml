<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-02-11T22:22:40-08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Akshay Trikha</title><subtitle></subtitle><entry><title type="html">Material Property Prediction Part 1: A Naive Approach</title><link href="http://localhost:4000/deep-learning/2024/02/08/material-property-prediction-1.html" rel="alternate" type="text/html" title="Material Property Prediction Part 1: A Naive Approach" /><published>2024-02-08T07:24:17-08:00</published><updated>2024-02-08T07:24:17-08:00</updated><id>http://localhost:4000/deep-learning/2024/02/08/material-property-prediction-1</id><content type="html" xml:base="http://localhost:4000/deep-learning/2024/02/08/material-property-prediction-1.html"><![CDATA[<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      processEscapes: true
    }
  });
</script>

<script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<p>Given its 3D coordinates &amp; atomic numbers of its constituent atoms, can you predict the atomization energy of a molecule? In this first attempt I’m going to try a naive approach by training a fully connected neueral network on a supervised task. To make things <em>even</em> more interesting I’ll be writing the neueral network from scratch using just numpy. My hope was this will give me a deeper and more satisfying understanding of how information flows.</p>

<h3 id="the-qm7-dataset">The QM7 dataset</h3>

<p>The QM7 dataset <a href="http://quantum-machine.org/datasets/">[1]</a> consists of 7165 molecules stable organic molecules that have up to 23 constituent atoms. It’s a subset of the larger GDB-13 dataset <a href="https://gdb.unibe.ch/downloads/">[2]</a> which consists of ~1 billion molecules - it’s nice to start with a computationally easier problem.</p>

<p>Each molecule has two fixed lengths matrixes describing it even if it consists of fewer than 23 atoms:</p>
<ol>
  <li><code class="language-plaintext highlighter-rouge">23 x 3</code> matrix <code class="language-plaintext highlighter-rouge">R</code> containing information about its atom positions</li>
  <li><code class="language-plaintext highlighter-rouge">23 x 1</code> vector <code class="language-plaintext highlighter-rouge">Z</code> containing the atomic numbers of the atoms in their respective positions.</li>
</ol>

<p>For e.g. a sample entry in the dataset looks like this:</p>

<pre><code class="language-Python">R = [[ 1.8897262 ,  0.        ,  0.        ],
    [ 4.6184907 ,  0.        ,  0.        ],
    [ 6.867567  ,  0.        ,  0.        ],
    [ 9.596332  ,  0.        ,  0.        ],
    [10.316582  ,  2.7353597 ,  0.15233083],
    [10.638704  ,  4.0255136 ,  2.673301  ],
    [12.828274  ,  3.4122975 ,  1.1119337 ],
    [ 1.1453441 ,  1.9292403 ,  0.02254443],
    [ 1.1450795 , -0.9839615 ,  1.6594819 ],
    [ 1.1450039 , -0.94497645, -1.6819507 ],
    [10.345476  , -0.8279646 , -1.7417984 ],
    [10.345608  , -1.0861768 ,  1.5949667 ],
    [ 9.660261  ,  3.832837  , -1.4499112 ],
    [10.418041  ,  2.9208362 ,  4.3819723 ],
    [10.137946  ,  6.002734  ,  2.8299594 ],
    [13.497936  ,  4.9887447 ,  0.21958618],
    [ 0.        ,  0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ],
    [ 0.        ,  0.        ,  0.        ]]

Z = [6., 6., 6., 6., 6., 6., 7., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,
    0., 0., 0., 0., 0., 0.]
</code></pre>

<p>Similar to handling images as input to computer vision models, we flatten the <code class="language-plaintext highlighter-rouge">R</code> and <code class="language-plaintext highlighter-rouge">Z</code> to be vectors with dim <code class="language-plaintext highlighter-rouge">92 x 1</code> for each molecule (92 = 23 rows x 4 columns). The flattening works because each entry in the input vector will always correspond to the same entry in either <code class="language-plaintext highlighter-rouge">R</code> or <code class="language-plaintext highlighter-rouge">Z</code>.</p>

<p>Our model’s output will be a scalar quantity representing the atomization energy of the molecule. <strong>TODO</strong>: write about mean + std stuff</p>

<h3 id="model-implementation">Model Implementation</h3>

<p>Let’s start with how we want the network to look like:</p>

<pre><code class="language-Python">layers = [
        input_layer,
        LinearLayer(dim_in=input_layer.num_out, dim_out=400),
        Tanh(),
        LinearLayer(400, 100),
        Tanh(),
        LinearLayer(100, 1),
        output_layer,
    ]
</code></pre>

<p>This seems reasonable. We’re projecting the input of length 92 into 400 dimensions, then 100, then to a final 1 dimension. Hopefully in between the connections the neural network will be able to learn an ok representation. Since the network is so small I don’t think choosing tanh will be much of a penalty over relu.</p>

<p>So what do the individual layers’ classes look like? Here’s an abstract class that contains the potential methods we would need for a layer.</p>

<pre><code class="language-Python">class Module:
    @abstractmethod
    def forward(self, X):
        """Forward pass to compute the output of the module."""
        pass

    @abstractmethod
    def backward(self, DY):
        """Backward pass to compute gradients with respect to module 
        parameters."""
        pass

    @abstractmethod
    def update(self, lr):
        """Update the parameters of the module."""
        pass

    # @abstractmethod
    # def average(self, nn, a):
    #     """Average the parameters of the module with another module."""
    #     pass  
</code></pre>

<p>Every layer needs way to propagate inferences with <code class="language-plaintext highlighter-rouge">forward()</code> and a way to backpropagate gradients <code class="language-plaintext highlighter-rouge">backward()</code>. We also need to a way to <code class="language-plaintext highlighter-rouge">update()</code> the weights and biases of a given layer. TODO: Finally, we also need to <code class="language-plaintext highlighter-rouge">average()</code>. With this template we’ll be able to implement the <code class="language-plaintext highlighter-rouge">Input</code>, <code class="language-plaintext highlighter-rouge">Linear</code>, <code class="language-plaintext highlighter-rouge">Output</code> layers and their <code class="language-plaintext highlighter-rouge">Sequential</code> container.</p>

<p><strong>1. Input</strong></p>

<p>The easiest layer is perhaps the input. It’s given a tuple of matrixes <code class="language-plaintext highlighter-rouge">(R, Z)</code> and all we need to do is flatten it and also calculate the layer’s output dimension. It may look like we’re doing duplicate computation while initializing and when forward() is called but in <code class="language-plaintext highlighter-rouge">__init__()</code> we need to calcualte the output dimension so that the next layer can initialize properly - and in <code class="language-plaintext highlighter-rouge">forward()</code> we also need to flatten. However <code class="language-plaintext highlighter-rouge">inp</code> and <code class="language-plaintext highlighter-rouge">x</code> can be of different shape as <code class="language-plaintext highlighter-rouge">inp</code> is whatever we initialize the nn with and <code class="language-plaintext highlighter-rouge">X</code> is probably a minibatch during training / inference.</p>

<pre><code class="language-Python">class Input(Module):
    def __init__(self, inp):
        """Initialize the input layer of the MLP
        Args:
            inp (tuple): tuple of numpy arrays (R, Z) of shape 
                         (nbatch, natoms, 3) and (nbatch, natoms)
        """
        R, Z = inp
        sample_in = np.concatenate([R, np.expand_dims(Z, -1)], axis=-1)
        self.num_out = sample_in.shape[-2] * sample_in.shape[-1]

    def forward(self, X):
        """Given input inp, perform a forward pass through layer"""
        R, Z = X
        rz = np.concatenate([R, np.expand_dims(Z, -1)], axis=-1)
</code></pre>

<p><strong>2. Linear</strong></p>

<p>The linear layer is where it gets a bit more interesting. Now we need a way to pass information both forward and backward as well as update the parameters. This class is long so lets break it up. We start by randomly initializing the weights and biases. TODO: Adjusting the learning rate by $\frac{1}{\sqrt{m}}$</p>

<pre><code class="language-Python">class Linear(Module):
    """Linear layer with weights W and bias B"""

    def __init__(self, m, n):
        """Initialize the weights as randomly sampled from normal distribution 
        and scale lr by 1/sqrt(m). Initialize the biases as zeros.

        Args:
            m (int): number of input features
            n (int): number of output features
        """
        # adjust lr by 1/sqrt(m) to account for scaling of weights
        self.lr = 1 / m**0.5
        self.W = np.random.normal(0, 1 / m**0.5, [m, n]).astype("float32")
        self.B = np.zeros([n]).astype("float32")

</code></pre>

<p>What does the output to the linear layer mean? It means that we’re trying to find the output $Y$ in $Y = WX + B$ where $W$ are the weights of the layer and $B$ its biases. Note that since $W$ and $B$ are matrixes we have to matrix multiply them. We store <code class="language-plaintext highlighter-rouge">self.X</code> because we use it again in the backward pass. Seems easy enough.</p>
<pre><code class="language-Python">    def forward(self, X):
        """Perform forward pass through linear layer"""
        self.X = X
        self.output = np.matmul(X, self.W) + self.B
        return self.output
</code></pre>

<p>Ok <code class="language-plaintext highlighter-rouge">backward()</code> is slightly tricky. The big idea is that somehow we want to measure how much each of the weights and biases in the layer affect the final loss. This will eventually help us in adjusting the parameters and pointing the model in a better direction at the end of the epoch.</p>

<p>For the weights: in mathematical terms we can say that we want to find $\frac{dL}{dW}$ where $L$ represents the loss - or how bad the model was compared to the ground truth. With a bit of chain rule we can decompose this into other derivatives we might know:</p>

<p>\[\frac{dL}{dW} = \frac{dL}{dY} \times \frac{dY}{dW}\]</p>

<p>a-ha! We know that since we’re in a linear layer and $Y = MX + B$  then</p>

<p>\[\frac{dY}{dW} = X\]</p>

<p>Well that’s convenient - since we stored <code class="language-plaintext highlighter-rouge">self.X</code> in the forward pass we can reuse it here. Finally, we’re given $\frac{dL}{dY}$ as a input to the function <code class="language-plaintext highlighter-rouge">dL</code> so we can write <code class="language-plaintext highlighter-rouge">self.dW = self.X.T @ dY</code>.</p>

<p>Now for the biases: now we want to find $\frac{dL}{dB}$. Similarly with chain rule:
\[\frac{dL}{dB} = \frac{dL}{dY} \times \frac{dY}{dB}\]</p>

<p>and</p>

<p>\[\frac{dY}{dW} = 1\]</p>

<p>huh. So <code class="language-plaintext highlighter-rouge">self.dB = dY</code>? Generally, yeah, but not so quick: beacuse we’ll likely be using a minibatch during training we’ll actually want to use <code class="language-plaintext highlighter-rouge">self.dB = np.sum(dY, axis=0)</code>.</p>

<p>Finally for calculating the gradient, we want to find an expression for how much the loss changes with respect to the input to this layer. At first this might seem a little weird but we must remember that the inputs we’re talking about to <em>this</em> layer are the outputs of the <em>previous</em> layer. In other words, we need a way to propagate the gradient backwards so that the previous layers can also use it.</p>

<p>\[\frac{dL}{dX} = \frac{dL}{dY} \times \frac{dY}{dX}\]</p>

<p>We know <code class="language-plaintext highlighter-rouge">dY</code> is given to us and $\frac{dY}{dX} = W$ so the expression is <code class="language-plaintext highlighter-rouge">self.grad = dY @ self.W.T</code>.</p>

<p>An important note about <code class="language-plaintext highlighter-rouge">update()</code> here is that we want to adjust the amount we update the weights and biases by the learning rate (sometimes referred to as $\alpha$). The usual analogy people use is that the gradient dictates which direction we want to go in and the learning rate tells us how big of a step to take. For more intuition about backpropagation I’d recommend checking out Andrej Karpathy’s excellent video <a href="https://www.youtube.com/watch?v=VMj-3S1tku0">[3]</a>.</p>

<pre><code class="language-Python">    def backward(self, dY):
        """Perform backward pass through linear layer"""
        self.dW = self.X.T @ dY

        # sum over all samples in batch
        self.dB = np.sum(dY, axis=0)

        # dL / dX = dL / dY @ dY / dX
        self.grad = dY @ self.W.T

        return self.grad

    def update(self, lr):
        """Update the weights and biases of linear layer"""
        self.W -= lr * self.lr * self.dW
        self.B -= lr * self.lr * self.dB

    # def average(self, nn, a):
    #     """Average the weights and biases of linear layer with another 
    #         linear layer"""
    #     self.W = a * nn.W + (1 - a) * self.W
    # self.B = a * nn.B + (1 - a) * self.B
</code></pre>

<p><strong>3. Tanh</strong></p>

<p>This layer is really straightforward. We need a nonlinear activiation function to learn nonlinear functions. So the <code class="language-plaintext highlighter-rouge">forward()</code> method is just using calling the tanh function like so <code class="language-plaintext highlighter-rouge">self.Y = np.tanh(X)</code>. Then for <code class="language-plaintext highlighter-rouge">backward()</code> we want to differentiate the function and lucky for us someone has already found it to be $1 - tanh^2$. We stored the output of <code class="language-plaintext highlighter-rouge">forward()</code> as <code class="language-plaintext highlighter-rouge">self.Y</code> which we can reuse in our backward to express the derivative as $1 - Y^2$. Then to propagate the gradient that was given to us from the previous layer we have $(1 - Y^2) \times dY$.</p>

<pre><code class="language-Python">class Tanh(Module):
    def forward(self, X):
        self.Y = np.tanh(X)
        return self.Y

    def backward(self, dY):
        self.grad = (1 - self.Y**2) * dY
        return self.grad
</code></pre>

<p><strong>4. Output</strong></p>

<p><strong>5. Sequential</strong></p>

<p>The <code class="language-plaintext highlighter-rouge">Sequential</code> class is our container for neatly packaging all of the network’s sublayers. Its <code class="language-plaintext highlighter-rouge">forward()</code> and <code class="language-plaintext highlighter-rouge">backward()</code> methods exists as orchestrators for how the information flows in the network. The only real difference between their implementations is that in <code class="language-plaintext highlighter-rouge">backward()</code> we have to remember to iterate backwards through the layers. The way we’ve implemented <code class="language-plaintext highlighter-rouge">update()</code> is analgous to <code class="language-plaintext highlighter-rouge">torch.optim.Optimizer.step()</code> in PyTorch and we would call it once we’ve found the gradients and want to then update all our paramters.</p>

<pre><code class="language-Python">class Sequential(Module):
    def __init__(self, modules):
        self.modules = modules

    def forward(self, X):
        """Given input X, perform a full forward pass through the MLP"""
        # iterate through layers and call forward() on output of each previous layer
        for module in self.modules:
            X = module.forward(X)

        return X

    def backward(self, dY):
        """Perform a full backward pass through the MLP.
        dY is gradient of the loss w.r.t the final output"""
        for module in reversed(self.modules):
            dY = module.backward(dY)

        return dY

    def update(self, lr):
        for m in self.modules:
            m.update(lr)

    # def average(self, nn, a):
    #     for m, n in zip(self.modules, nn.modules):
    #         m.average(n, a)
</code></pre>

<p>And there you have it! That’s all the layers you’ll need to train a dense network. Pretty amazing how little code there is right.</p>

<h4 id="resources">Resources:</h4>

<ul>
  <li><a href="http://quantum-machine.org/datasets/">[1] QM7 Dataset</a></li>
  <li><a href="https://gdb.unibe.ch/downloads/">[2] GDB-13</a></li>
  <li><a href="https://www.youtube.com/watch?v=VMj-3S1tku0">[3] Andrej Karpathy’s Intro to NNs and Backpropagation</a></li>
</ul>]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Universal Mask Finding</title><link href="http://localhost:4000/2023/09/24/universal-mask-finding.html" rel="alternate" type="text/html" title="Universal Mask Finding" /><published>2023-09-24T13:29:00-07:00</published><updated>2023-09-24T13:29:00-07:00</updated><id>http://localhost:4000/2023/09/24/universal-mask-finding</id><content type="html" xml:base="http://localhost:4000/2023/09/24/universal-mask-finding.html"><![CDATA[<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/fast_mask_finding.png" alt="ML defect detection" />
        <figcaption>Fig 1. Mask finding of a chocolate bar in the making</figcaption>
    </div>
    <br />
</figure>

<p><br />
<strong>Preface</strong></p>

<p>I write using analogy of working for Willy Wonka’s chocolate factory to demonstrate that this method could be used in any manufacturing context that uses computer vision for defect detection. It’s also just fun to think about mass producing chocolate.</p>

<p><br />
<strong>Background</strong></p>

<p>You recently got hired as an Oompa Loompa software engineer working at Willy Wonka’s chocolate factory on a team that images chocolate accross the various stages of the manufacturing line in order to find defects. They use all sorts of fancy cameras: 2D, 3D, radiographs - they know their stuff. This approach lets them both find defects quicker than waiting till the end of the factory line to find out something’s bad, and also allows scrapping defective material much earlier in the manufacturing process to cut losses.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/defect_detection.gif" alt="ML defect detection" />
        <figcaption>Fig 2. Defect Detection</figcaption>
    </div>
    <br />
</figure>

<p>However, before we try and find defects in an image of a bar we’ve got to first find an area to search through for defects – otherwise a defect model might get confused and flag the background as defective. We’d also want constrain the defect search space as much as possible to make the defect model more efficient at flagging defects. I refer to this specific semantic segmentation task as “mask finding”.</p>

<p>For example, if your human neural network saw images in Fig 2. of bars-in-the-making, what features would you think distinguish it from the background? Maybe that a bar has four corners, is a different color than the background, and has a different surface consistency. If we as huamns can distinguish chocolate from it’s background we could probably write a rules based program for finding a segmentation mask, but the types of input images often rapidly changes:</p>

<ul>
  <li>Oompa Loompa scientists may iterate on the product and future chocolates might have different dimensions</li>
  <li>imaging conditions of inspection stations may change (different light color, angle)</li>
  <li>we may want to use a different type of camera (laser profiler, darkfield black boxes)</li>
</ul>

<p>All of which leads to a dizzying combination of image possibilities and you’d be playing a game of cat-and-mouse to keep up by writing separate image processing pipelines for each variation.</p>

<!-- It would be maybe 10x more difficult to build and maintain 40 different traditional image processing pipelines with human coded logic to find masks compared to using one ML model. -->

<p><br />
<strong>ML</strong></p>

<p>So then how do we scale this process up for hundreds of thousands of images a month across multiple image and camera types? Universal Mask Finding is an ML model I trained while I was at the chocolate factory to solve this problem. The magic of deep learning is that we don’t explicitly program the model to find the features we just mentioned, we ask the model to hopefully learn them automatically across our dataset.</p>

<p>The model is based on an off-the-shelf <a href="https://paperswithcode.com/method/u-net#:~:text=U%2DNet%20is%20an%20architecture,architecture%20of%20a%20convolutional%20network">U-Net</a> segmentation architecture with pretrained resnet34 weights and infers a pixel-level segmentation mask. That means, for every pixel in an image the model predicts the probability that it belongs to the foreground, or background. I fine-tuned a pretrained model originally trained on the ImageNet task with the relevant images of chocolates. More about transfer learning <a href="/deep-learning/2023/07/04/transfer-learning.html">here</a>.</p>

<p>We measure the performance of a segmentation model based on the Intersection over Union score, or IOU, which is a fancy way of saying what % of the prediction overlapped with a ground truth label.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/IOU.gif" alt="mIOU" />
        <figcaption>Fig 2. Measuring performance with IOU</figcaption>
    </div>
    <br />
</figure>

<p>We can also do interesting things here with the inference probabilities like setting a lower <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html" target="\_blank">threshold</a> for the mask class like 0.3 to increase the recall (but at the cost of decreased precision).</p>

<p><br />
<strong>Generalization</strong></p>

<p>Often in machine learning we want a model to be able to generalize on a given task as much as possbile, and that’s probably a good marker that it’s learning something about the underlying nature of the problem. Somewhat counterintuitively, I found that I get the best performance by allowing a single model to learn features across all the varying “image types”. Examples of things I would separate into different “image type” categories include different cameras, backgrounds, and components.</p>

<p>To prove the model generalizes well I ran a simple study where I gradually increased the number of image types in the training set, and measured performance against unseen image types.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/generalization.png" alt="generalization" />
        <figcaption>Fig 3. Generalization</figcaption>
    </div>
    <br />
</figure>

<p>And if we include all image types (dotted lines) during training we can close the gap to virtually perfect. This is particularly nice for two reasons:</p>

<ol>
  <li>because there is a lot of uncertainty in incoming images in production</li>
  <li>because this approach saves Willy Wonka $$$ per month in server costs compared to deploying 40 different models for each image type</li>
</ol>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/image_types.gif" alt="image-types" />
        <figcaption>Fig 4. Different image types encountered in production</figcaption>
    </div>
    <br />
</figure>

<p><br />
<strong>Building Trust: Classification Reviewer Model</strong></p>

<p>Some fellow Oompa Loompa cocoa scientists asked me how they could trust my mask finding model’s inferences - a very fair question. I tried developing rule based metrics to see if a mask had 4 corners, straight lines etc. but it turned out to be a difficult task as no one criteria would capture the complexities of judging mask finding quality. I realized that a deep learning model classifier would outperform any handwritten metrics I could come up with. This new classifier would run after each segmentation prediction and would have 3 classes:</p>

<figure style="display: flex; gap: 4.9%; text-align: center;">
    <div style="width: 30%;">
        <img src="http://localhost:4000/assets/mask-finding/grade_1.png" alt="ML defect detection" />
        <figcaption>Grade 1: inferred mask is perfect</figcaption>
    </div>
    <div style="width: 30%;">
        <img src="http://localhost:4000/assets/mask-finding/grade_2.png" alt="ML defect detection" />
        <figcaption>Grade 2: inferred mask is imperfect but can still be used for downstream defect detection</figcaption>
    </div>
    <div style="width: 30%;">
        <img src="http://localhost:4000/assets/mask-finding/grade_3.png" alt="ML defect detection" />
        <figcaption>Grade 3: inferred mask has poor quality and can't be trusted for defect defection</figcaption>
    </div>
</figure>

<p>Because I already had ground truth human labels for the segmentation task I already had a great set of examples for Grade 1. All I needed to do was find a handful Grade 2 and 3 failures and to my surprise the classifier had an <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">AUC</a> of 0.995! We could now use the classifier’s predictions to understand whether or not to trust the segmentation model.</p>

<p><br />
<strong>Building Trust: Historical Performance</strong></p>

<p>To further build trust in the model I monitor how each image type, which is demarked by a metadata tag in the dataset, performs over time, making sure that all their respective mIOUs are close to 1. I also periodically check my historical data log to inspect how the model is doing in production and shuffle the dataset with the latest examples of failures. It’s very difficult for me to perfectly monitor this model in production and so I also rely on my fellow Oompa Loompa scientists and production managers to notify me if they notice a drop in inference quality.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/mask-finding/historical_performance.png" alt="historical-performance" />
        <figcaption>Fig 5. Tracking historical performance </figcaption>
    </div>
    <br />
</figure>

<p>In general the number of image types for the model to run on have only been growing so I typicially also evaluate a new model based on a previous iteration’s dataset just to double check that I’m not noticing a drop in quality for older image types. It’s also fun to evaluate an older iteration on the latest model’s test dataset to quantify the improvement.</p>

<p>I’m proud this ML system has been robust while integrating new cameras and chocolate types and was able to scale to help the chocolate factory grow while delivering the best chocolate possible.</p>

<!-- ![Fast Mask Finding](../../../assets/fast_mask_finding.gif) -->]]></content><author><name></name></author><summary type="html"><![CDATA[Fig 1. Mask finding of a chocolate bar in the making]]></summary></entry><entry><title type="html">Transfer Learning in Computer Vision</title><link href="http://localhost:4000/deep-learning/2023/07/04/transfer-learning.html" rel="alternate" type="text/html" title="Transfer Learning in Computer Vision" /><published>2023-07-04T09:06:17-07:00</published><updated>2023-07-04T09:06:17-07:00</updated><id>http://localhost:4000/deep-learning/2023/07/04/transfer-learning</id><content type="html" xml:base="http://localhost:4000/deep-learning/2023/07/04/transfer-learning.html"><![CDATA[<p>Recall when beginning to train a neural network its weights might be initialized randomly. What if you could start training with a leg up because the network already has some useful information inside of it? Transfer learning <em>is</em> that leg up where you can repurpose models trained on similar tasks and use them for your specific task instead of training from scratch.</p>

<p>Benefits include:</p>

<ul>
  <li>training faster</li>
  <li>less training data</li>
  <li>boosts generalization</li>
  <li>might improve accuracy</li>
</ul>

<figure>
    <br />
    <img src="http://localhost:4000/assets/transfer-learning/transfer-learning.png" alt="transfer learning diagram" />
    <figcaption style="text-align: center">diagram inspired by Stanford CS 329P 2021 slides</figcaption>
    <br />
</figure>

<p>Above we see an example of using ImageNet pre-trained weights to classify objects from the Fashion MNIST dataset. A bit of an overkill example, but you get the point.</p>

<p>There are three main ways you might go about transfer learning:</p>

<ol>
  <li>
    <p>Use the pre-trained model as a pre-trained feature extractor by freezing its hidden layers (and replacing its head with your own). This works well when your task isn’t too different from the pre-trained model’s task.</p>
  </li>
  <li>
    <p>Finetune the pre-trained model completely by not freezing any layers</p>
  </li>
  <li>
    <p>Finetune but freeze some number layers. There are two main risks with this approach: (1) the higher-level early neurons are more specialized than we expected, and (2) splitting &amp; combining two arbitrary layers causes a mismatch in learned features that cannot be relearned by earlier layers.</p>
  </li>
</ol>

<p>Thanks to the open-source community there are many places you can find pre-trained models, like PyTorch Hub, HuggingFace, and TensorFlow Hub to name a few.</p>

<h3 id="segmentation-example">Segmentation Example</h3>

<h4 id="1-loading-the-model">1. Loading the Model</h4>

<p>I’ve used the DeepLabv3 model from <a href="https://pytorch.org/hub/pytorch_vision_deeplabv3_resnet101/">Pytorch Hub</a> and gotten some pretty great results. We first start by loading the pre-trained weights into the model:</p>

<pre><code class="language-Python">import torchvision

# there are three model sizes: MobileNet, ResNet50, and ResNet101
weights = torchvision.models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT
model = torchvision.models.segmentation.deeplabv3_resnet50(weights=weights).to(device)
</code></pre>

<p>Then we need to replace the classifier layer but to do that we need to know its input dimension. The easiest way I’ve found to do that is to use <code class="language-plaintext highlighter-rouge">torchinfo.summary()</code>:</p>

<pre><code class="language-Python">from torchinfo import summary

summary(
    model=model,
    input_size=(2, 3, 1024, 1024),
    col_names=["input_size", "output_size", "num_params", "trainable"],
    col_width=20,
    row_settings=["var_names"]
)
</code></pre>

<p>which prints out:</p>

<pre><code class="language-#1">===========================================================================================================================
Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable
===========================================================================================================================
DeepLabV3 (DeepLabV3)                              [2, 3, 1024, 1024]   [2, 2, 1024, 1024]   --                   True
├─IntermediateLayerGetter (backbone)               [2, 3, 1024, 1024]   [2, 2048, 128, 128]  --                   True
│    └─Conv2d (conv1)                              [2, 3, 1024, 1024]   [2, 64, 512, 512]    9,408                True
│    └─BatchNorm2d (bn1)                           [2, 64, 512, 512]    [2, 64, 512, 512]    128                  True
│    └─ReLU (relu)                                 [2, 64, 512, 512]    [2, 64, 512, 512]    --                   --
│    └─MaxPool2d (maxpool)                         [2, 64, 512, 512]    [2, 64, 256, 256]    --                   --
│    └─Sequential (layer1)                         [2, 64, 256, 256]    [2, 256, 256, 256]   --                   True
│    │    └─Bottleneck (0)                         [2, 64, 256, 256]    [2, 256, 256, 256]   75,008               True
│    │    └─Bottleneck (1)                         [2, 256, 256, 256]   [2, 256, 256, 256]   70,400               True
│    │    └─Bottleneck (2)                         [2, 256, 256, 256]   [2, 256, 256, 256]   70,400               True
.                                                           .                                                       .
.                                                           .                                                       .
.                                                           .                                                       .
├─DeepLabHead (classifier)                         [2, 2048, 128, 128]  [2, 2, 128, 128]     --                   True
│    └─ASPP (0)                                    [2, 2048, 128, 128]  [2, 256, 128, 128]   --                   True
│    │    └─ModuleList (convs)                     --                   --                   15,206,912           True
│    │    └─Sequential (project)                   [2, 1280, 128, 128]  [2, 256, 128, 128]   328,192              True
│    └─Conv2d (1)                                  [2, 256, 128, 128]   [2, 256, 128, 128]   589,824              True
│    └─BatchNorm2d (2)                             [2, 256, 128, 128]   [2, 256, 128, 128]   512                  True
│    └─ReLU (3)                                    [2, 256, 128, 128]   [2, 256, 128, 128]   --                   --
│    └─Conv2d (4)                                  [2, 256, 128, 128]   [2, 2, 128, 128]     514                  True
===========================================================================================================================
Total params: 39,633,986
Trainable params: 39,633,986
Non-trainable params: 0
Total mult-adds (T): 1.31
===========================================================================================================================
Input size (MB): 25.17
Forward/backward pass size (MB): 17650.16
Params size (MB): 158.54
Estimated Total Size (MB): 17833.87
===========================================================================================================================
</code></pre>

<p>From the <code class="language-plaintext highlighter-rouge">DeepLabHead (classifier)</code> line which has shape <code class="language-plaintext highlighter-rouge">[batch, in_channels, height, width]</code> we can see that we need <code class="language-plaintext highlighter-rouge">in_channels=2048</code> for our new classifier.</p>

<pre><code class="language-Python"># modify classifier layer for desired number of classes
# number for in_channels was found by examining the model architecture
model.classifier = DeepLabHead(in_channels=2048, num_classes=NUM_CLASSES)
</code></pre>

<h4 id="2-tensor-shapes">2. Tensor Shapes</h4>

<p>Then let’s try and understand the model’s output. From its documentation: <em>“The model returns an</em> <code class="language-plaintext highlighter-rouge">OrderedDict</code> <em>with two Tensors that are of the same height and width as the input Tensor …</em> <code class="language-plaintext highlighter-rouge">output['out']</code> <em>contains the semantic masks, and</em> <code class="language-plaintext highlighter-rouge">output['aux']</code> <em>contains the auxiliary loss values per-pixel.”</em></p>

<pre><code class="language-Python"># output['out'] is what we really want
# output is a tensor with shape [batch_size, num_classes, height, width]
output = model(X_train.to(device))["out"]
</code></pre>

<p>In order to calculate the loss we have to massage the tensors a bit. I chose <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss()</code> as the loss function and its documentation allows the input to have shape <code class="language-plaintext highlighter-rouge">[batch_size, num_classes, height, width]</code> but its target must have shape <code class="language-plaintext highlighter-rouge">[batch_size, height, width]</code> so with help from the <code class="language-plaintext highlighter-rouge">einops</code> package:</p>

<pre><code class="language-Python">from einops import rearrange

# rearrange target
target = rearrange(target, "bat cla height width -&gt; (bat cla) height width")

# calculate loss
loss = loss_fn(output, target)
</code></pre>

<p>Now we notice that for each batch there is a dimension in the first index of the tensor for <code class="language-plaintext highlighter-rouge">logit(false)</code> and <code class="language-plaintext highlighter-rouge">logit(true)</code> which is redundant. We can just keep <code class="language-plaintext highlighter-rouge">logit(true)</code> for each batch, take that softmax, binarize the predictions, and finally calculate accuracy.</p>

<pre><code class="language-Python"># modify output to be in format [logit(true)] for each sample
output = output[:, 1, :, :]

# take softmax
output = nn.functional.softmax(output, dim=1)

# binarize predictions &amp; calculate accuracy
y_pred = (output &gt; 0.5).type(torch.int32)
accuracy_fn = torchmetrics.Accuracy(task="binary", num_classes=NUM_CLASSES)
accuracy = accuracy_fn(y_pred)
</code></pre>

<h4 id="3-wrapping-up">3. Wrapping Up</h4>

<p>With the loss and accuracy for a batch we can go ahead and train our model. The only other trouble I had with with my custom dataset was that I didn’t realize <code class="language-plaintext highlighter-rouge">torchvision.transforms.ToTensor()</code> automatically divides a tensor by 256 so make sure your data is correctly scaled!</p>

<p>For experiment tracking I used <a href="https://wandb.ai/site">wandb</a> which was super simple to set up and let me easily visualize how tweaking some hyperparameters like batch size affected this model’s training. You can find how I created the dataset class, training loops, and experiment tracking for segmentation transfer learning in my <a href="https://github.com/akshaytrikha/transfer-learning/blob/main/segmentation/scripts/">GitHub</a> repository.</p>

<h3 id="classification-example">Classification Example</h3>

<p>Transfer learning for a classification task is virtually the same as segmentation and slightly easier. We start again by loading the pre-trained weights into the model and replacing the classifier layer so that it has the right input and output dimensions for our target problem.</p>

<pre><code class="language-Python">import torchvision

# there are multiple other model sizes
weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT
model = torchvision.models.efficientnet_b0(weights=weights).to(device)

# if you wanted to freeze base layers
for param in model.features.parameters():
    param.requires_grad = False

# modify classifier layer for number of classes
# added dropout for more robustness
model.classifier = nn.Sequential(
    torch.nn.Dropout(p=0.2, inplace=True),
    nn.Linear(in_features=CLASSIFIER_IN_FEATURES, out_features=NUM_CLASSES),
).to(device)
</code></pre>

<p>The only tricky thing here is calculating <code class="language-plaintext highlighter-rouge">CLASSIFIER_IN_FEATURES</code>, which was found by printing out the model’s layers using <code class="language-plaintext highlighter-rouge">torchinfo.summary()</code>. The training step is more straightforward than the segmentation example:</p>

<pre><code class="language-Python"># forward pass
# outputs are in the format [logit(true)] for each sample
# logit = log(unnormalized probability)
outputs = model(X_train.to(device))

# calculate loss &amp; accuracy
loss = loss_fn(outputs, y_train)
accuracy = accuracy_fn(outputs, y_train)
</code></pre>

<p>There you have it, now you can use transfer learning for both segmentation and classification tasks.</p>

<h4 id="resources">Resources:</h4>

<ul>
  <li><a href="https://cs231n.github.io/transfer-learning">https://cs231n.github.io/transfer-learning</a></li>
  <li><a href="https://arxiv.org/pdf/1411.1792.pdf">How transferable are features in deep neural networks?</a></li>
  <li><a href="https://www.learnpytorch.io/06_pytorch_transfer_learning/">https://www.learnpytorch.io/06_pytorch_transfer_learning/</a></li>
</ul>]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[Recall when beginning to train a neural network its weights might be initialized randomly. What if you could start training with a leg up because the network already has some useful information inside of it? Transfer learning is that leg up where you can repurpose models trained on similar tasks and use them for your specific task instead of training from scratch.]]></summary></entry><entry><title type="html">Trump or Computer Dump?</title><link href="http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html" rel="alternate" type="text/html" title="Trump or Computer Dump?" /><published>2023-04-23T14:53:17-07:00</published><updated>2023-04-23T14:53:17-07:00</updated><id>http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump</id><content type="html" xml:base="http://localhost:4000/deep-learning/2023/04/23/trump-or-computer-dump.html"><![CDATA[<script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>

<gradio-app src="https://akshaytrikha-gpt2-trump.hf.space"></gradio-app>

<p>In early 2019 OpenAI published some <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">research</a> showing that large language models, when trained on colossal amounts of text, begin to behave in unpredictably intelligently on tasks they weren’t originally trained on. Humorously, part of the original dataset for the model was scraped from Reddit - which isn’t always known to be the home of constructive conversation and accurate information so it’s even more surprising how good their model was!</p>

<p>In 2020, while taking an NLP, course I thought it would be fun to finetune GPT-2 on a corpus of then President Trump’s tweets seeing as he has such a distinct style and voice. I obtained his tweets from the super convenient <a href="https://www.thetrumparchive.com/">Trump Twitter Archive</a> which contained ~56,500 of his tweets from 2009 until he was banned from the platform on January 8th, 2021.</p>

<p>The original project took me around 3 weeks if I remember correctly and this weekend as I was dog sitting I remembered that project and got excited about trying to recreate it with the latest tools available, namely HuggingFace. It was awesome to have a benchmark to compare against to measure and truly appreciate the efficiency and ergonomics of HuggingFace’s transformers library. I was able to replicate the finetuning through a short <a href="https://github.com/akshaytrikha/GPT-2-Trump/blob/master/huggingface_trump.ipynb">notebook</a> + host it using gradio on a HuggingFace space in just a couple of days.</p>

<h3 id="finetuning-with-huggingface-transformers">Finetuning with HuggingFace Transformers</h3>

<p>First I defined some hyperparams for training:</p>

<pre><code class="language-Python"># define some params for model
batch_size = 8
epochs = 15
learning_rate = 5e-4
epsilon = 1e-8
warmup_steps = 1e2
sample_every = 100  # produce sample output every 100 steps
max_length = 140  # max length used in generate() method of model
</code></pre>

<p>Assuming the twitter data is downloaded and preprocessed, we need to tokenize the tweets (which means assign a unique <code class="language-plaintext highlighter-rouge">int</code> to every word or character) and load them into a dataloader. The <code class="language-plaintext highlighter-rouge">AutoTokenizer</code> is really nice because it uses the right tokenizer for any specific model available on HuggingFace if you give it a model name e.g.<code class="language-plaintext highlighter-rouge">"gpt2-medium"</code>.</p>

<pre><code class="language-Python">from transformers import (
	AutoTokenizer
    TextDataset,
    DataCollatorForLanguageModeling
)

# create tokenized datasets
tokenizer = AutoTokenizer.from_pretrained(
    "gpt2-medium",
    pad_token='&lt;|endoftext|&gt;'
)

# custom load_dataset function because there are no labels
def load_dataset(train_path, dev_path, tokenizer):
    block_size = 128
    # block_size = tokenizer.model_max_length

    train_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=train_path,
          block_size=block_size)

    dev_dataset = TextDataset(
          tokenizer=tokenizer,
          file_path=dev_path,
          block_size=block_size)

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer, mlm=False,
    )
    return train_dataset, dev_dataset, data_collator

train_dataset, dev_dataset, data_collator = load_dataset(
	train_path, dev_path, tokenizer
)
</code></pre>

<p>Now we can instantiate the model, optimizer, and learning rate scheduler. I’d never experimented with dynamically changing the learning rate during training so using <code class="language-plaintext highlighter-rouge">get_linear_schedule_with_warmup()</code> was exciting. The resulting learning rate vs. epochs graph looked like this:</p>

<figure>
    <br />
	<div style="text-align: center">
		<img src="http://localhost:4000/assets/trumpdump/learning-rate.png" alt="linear learning rate schedule" style="width: 90%" />
	</div>
    <br />
</figure>

<p>which shows a delay in reaching the <code class="language-plaintext highlighter-rouge">learning_rate</code> by <code class="language-plaintext highlighter-rouge">num_epoch_steps</code> and then a slow decrease until the nth epoch.</p>

<pre><code class="language-Python">from transformers import (
    AutoModelWithLMHead,
    get_linear_schedule_with_warmup,
)

# AutoModelWithLMHead will pick GPT-2 weights from name
model = AutoModelWithLMHead.from_pretrained(
	model_name,
	cache_dir=Path('cache').resolve()
)

# necessary because of additional bos, eos, pad tokens to embeddings
model.resize_token_embeddings(len(tokenizer))

# create optimizer and learning rate schedule
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=epsilon)

training_steps = len(train_dataset) * epochs

# adjust learning rate during training
scheduler = get_linear_schedule_with_warmup(
	optimizer,
	num_warmup_steps = warmup_steps,
	num_training_steps = training_steps
)
</code></pre>

<p>Finally we can instantiate a <code class="language-plaintext highlighter-rouge">TrainingArguments</code> object which holds hyperparams and then run training from the <code class="language-plaintext highlighter-rouge">Trainer</code> object. This also prints out a pretty printed loss vs. step table.</p>

<pre><code class="language-Python">from transformers import (
    Trainer,
    TrainingArguments,
)

training_args = TrainingArguments(
    output_dir="./gpt-2-medium-trump",
    num_train_epochs=epochs,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    eval_steps = 400, # n update steps between two evaluations
    save_steps=800, # n steps per model save
    warmup_steps=500, # n warmup steps for learning rate scheduler
)

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
)

# train &amp; save model run after done
trainer.train()
trainer.save_model()
</code></pre>

<h3 id="inference-with-huggingface-transformers">Inference with HuggingFace Transformers</h3>

<p>Inference was even easier than training as all you need to do is set up a <code class="language-plaintext highlighter-rouge">pipeline</code> for the model:</p>

<pre><code class="language-Python">from transformers import pipeline

trump = pipeline(
	"text-generation",
	model="./gpt-2-medium-trump",
	tokenizer=tokenizer,
	config={"max_length":max_length}  # n tokens
)
</code></pre>

<p>Trump at your fingertips</p>

<pre><code class="language-Python">In:  trump("Today I'll be")[0]["generated_text"]

Out: "Today I'll be rallying w/ @FEMA, First Responders, Law Enforcement,
and First Responders of Puerto Rico to help those most affected by the
#IrmaFlood.https://t.co/gsFSghkmdM"
</code></pre>

<h4 id="resources">Resources:</h4>

<ul>
  <li><a href="https://huggingface.co/learn/nlp-course/chapter3/3?fw=pt">HuggingFace Trainer</a></li>
  <li><a href="https://huggingface.co/gpt2">GPT-2 Model Card</a></li>
  <li><a href="https://github.com/akshaytrikha/GPT-2-Trump/blob/master/huggingface_trump.ipynb">GPT-2 Trump notebook</a></li>
</ul>]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Neural Style Transfer Webcam</title><link href="http://localhost:4000/deep-learning/2021/07/16/neural-style-transfer-webcam.html" rel="alternate" type="text/html" title="Neural Style Transfer Webcam" /><published>2021-07-16T05:50:00-07:00</published><updated>2021-07-16T05:50:00-07:00</updated><id>http://localhost:4000/deep-learning/2021/07/16/neural-style-transfer-webcam</id><content type="html" xml:base="http://localhost:4000/deep-learning/2021/07/16/neural-style-transfer-webcam.html"><![CDATA[<h3 id="overview">Overview</h3>

<p>I wanted to learn how to build and deploy an ML system in the real world and thought this would be a fun place to start. You can find my project’s code <a href="https://github.com/akshaytrikha/style-transfer">here</a>.</p>

<figure>
    <br />
    <div style="text-align: center;">
        <img src="http://localhost:4000/assets/style-transfer/style-transfer.gif" alt="style transfer demo gif" style="width: 100%" />
        <figcaption style="text-align: center">
            Try the demo yourself <a href="https://akshaytrikha.github.io/style-transfer/" target="_blank" rel="noopener noreferrer">here</a>
        </figcaption>
    </div>
    <br />
</figure>

<!-- <figure>
    <br>

        <img src="http://localhost:4000/assets/mask-finding/fast_mask_finding.png" alt="ML defect detection"/>
        <figcaption>Fig 1. Mask Finding</figcaption>

    <br>
</figure> -->

<p>Using your webcam as input, this project generates stylized images in 400ms intervals. It uses two pretrained Tensorflow.js neural networks, sourced from Reiichiro Nakano’s <a href="https://github.com/reiinakano/arbitrary-image-stylization-tfjs">arbitrary-image-stylization-tfjs</a> repo.</p>

<ol>
  <li>The first network is used to learn the style of a given image and generate a style representation.</li>
  <li>The second network is then used for style transfer, or using the style representation to generate a stylized output image.</li>
</ol>

<p>The original models were actually regular tf models, but Reiichiro distilled them into smaller networks so they would run faster. For a more detailed breakdown of how the networks work, check out his blog <a href="https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/">post</a>.</p>

<h3 id="building-the-react-app">Building the React App</h3>

<p>I used Nicholas Renotte’s <a href="https://github.com/nicknochnack/ReactComputerVisionTemplate">ReactComputerVisonTemplate</a> repo as a template for interfacing with a webcam and rendering output back on screen. The big idea is to use the <code class="language-plaintext highlighter-rouge">react-webcam</code> component to take screenshots, generate their style representations, and then display their stylized images. In pseudo-ish code the entire app is basically:</p>

<pre><code class="language-JavaScript">import Webcam from "react-webcam";

const predict = async () =&gt; {
    // First wait for models to load
    await loadModels();
    // init style image and generate style representation
    await initStyleImage();

    // every 400ms
    setInterval(() =&gt; {
        captureScreenshot();
        generateStylizedImage();
    }, 400);
}
</code></pre>

<h3 id="lessons-learned">Lessons learned</h3>

<ol>
  <li>
    <p>A very fun fact about using TensorFlow.js is that your browser is locally running inference, and all your data is kept on your device.</p>
  </li>
  <li>
    <p>You have to do a lot of things asynchronously e.g. fetching the models, loading the models, running inference. Timing all of this was how I decided to introduce the delay of 400ms.</p>
  </li>
  <li>
    <p>tfjs is slightly different to using tf in python in small ways. You have to use methods like <code class="language-plaintext highlighter-rouge">tf.ready().then(() =&gt; {})</code> that returns a promise when tfjs is ready (if the WebGL backend is ready).</p>
  </li>
  <li>
    <p>I used the html <code class="language-plaintext highlighter-rouge">canvas</code> element to display images and would use <code class="language-plaintext highlighter-rouge">document.getElementById("element-name").src = ...</code> directly to update an element. I standardized the display sizes in px to make it easier to deal with.</p>
  </li>
  <li>
    <p>To make them persistent I stored the model weights in the repo itself so that they’re accessible with e.g. <code class="language-plaintext highlighter-rouge">transferModel = await tf.loadGraphModel(process.env.PUBLIC_URL + '/models/style-prediction/model.json')</code></p>
  </li>
</ol>

<h4 id="resources">Resources:</h4>

<ul>
  <li><a href="https://github.com/reiinakano/arbitrary-image-stylization-tfjs">https://github.com/reiinakano/arbitrary-image-stylization-tfjs</a></li>
  <li><a href="https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/">https://magenta.tensorflow.org/blog/2018/12/20/style-transfer-js/</a></li>
  <li><a href="https://styletransfer.art">https://styletransfer.art</a></li>
</ul>]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Barium Titanate Permittivity &amp;amp; Image Processing</title><link href="http://localhost:4000/2021/07/14/bto-image-processing.html" rel="alternate" type="text/html" title="Barium Titanate Permittivity &amp;amp; Image Processing" /><published>2021-07-14T14:53:17-07:00</published><updated>2021-07-14T14:53:17-07:00</updated><id>http://localhost:4000/2021/07/14/bto-image-processing</id><content type="html" xml:base="http://localhost:4000/2021/07/14/bto-image-processing.html"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Bias-Expressivity Trade-off</title><link href="http://localhost:4000/deep-learning/2019/11/09/bias-expressivity-tradeoff.html" rel="alternate" type="text/html" title="The Bias-Expressivity Trade-off" /><published>2019-11-09T13:53:17-08:00</published><updated>2019-11-09T13:53:17-08:00</updated><id>http://localhost:4000/deep-learning/2019/11/09/bias-expressivity-tradeoff</id><content type="html" xml:base="http://localhost:4000/deep-learning/2019/11/09/bias-expressivity-tradeoff.html"><![CDATA[]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">The Futility of Bias-Free Learning and Search</title><link href="http://localhost:4000/deep-learning/2019/07/25/futility-of-bias-free-learning-and-search.html" rel="alternate" type="text/html" title="The Futility of Bias-Free Learning and Search" /><published>2019-07-25T14:53:17-07:00</published><updated>2019-07-25T14:53:17-07:00</updated><id>http://localhost:4000/deep-learning/2019/07/25/futility-of-bias-free-learning-and-search</id><content type="html" xml:base="http://localhost:4000/deep-learning/2019/07/25/futility-of-bias-free-learning-and-search.html"><![CDATA[]]></content><author><name></name></author><category term="deep-learning" /><summary type="html"><![CDATA[]]></summary></entry></feed>