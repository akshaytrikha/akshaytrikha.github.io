---
layout: post
title: "(Bitter) Lessons from Scaling Machine Learning Interatomic Potentials"
date: 2025-09-25 13:11:17 -0000
categories: Research
thumbnail: /assets/thumbnails/scaling.jpeg
tldr: "Compared a plain transformer encoder to EquiformerV2 and studied their scaling laws"
redirect_from: /deep-learning/2025/09/25/scaling.html
---

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
</script>

**TL;DR:** We tested whether a plain transformer encoder (no equivariance, no periodic boundary conditions) can learn interatomic forces as data & params scale vs. EquiformerV2. EqV2 followed clean power laws and hit much lower loss. My transformers plateaued hard while learning forces, but observing their attention weights shows they learned graph structure as an emergent behavior. I share the wins & failures.

For EquiformerV2:
<style>
@media (max-width: 768px) {
  .desktop-only { display: none; }
}
@media (min-width: 769px) {
  .mobile-only { display: none; }
}
#root {
  min-height: auto !important;
}
#root .app-container {
  min-height: auto !important;
}
.mobile-only .MathJax_Display {
  text-align: left !important;
  margin-left: 0 !important;
  padding-left: 0 !important;
}
</style>

<div class="desktop-only" markdown="1">
$$
\begin{aligned}
&\quad\text{Parameter scaling law:} &&L \approx 7.76\times 10^{2}\, P^{-0.383} \\
&\quad\text{Compute scaling law:} &&L \approx 4.99\times 10^{5}\, C^{-0.339} \\
&\quad\text{Data scaling law:} &&L \approx 6.47\times 10^{1}\, D^{-0.242}
\end{aligned}
$$

For Transformer:
$$\text{Power laws not clean enough}$$
</div>

<div class="mobile-only" markdown="1">
$$
\begin{aligned}
&\text{Parameter scaling law:} \; L \approx 7.76\times 10^{2}\, P^{-0.383} \\
&\text{Compute scaling law:} \; L \approx 4.99\times 10^{5}\, C^{-0.339} \\
&\text{Data scaling law:} \; L \approx 6.47\times 10^{1}\, D^{-0.242}
\end{aligned}
$$

For Transformer: power laws not clean enough.
</div>

Code is open sourced [here](https://github.com/akshaytrikha/materials-scaling). Work was done alongside [Eric Weiner](https://www.linkedin.com/in/eric-weiner-7602b916a/), [Park Szachta](https://www.linkedin.com/in/parkszachta/), [Advait Gosai](https://www.linkedin.com/in/advaitgosai/), and [Kyle Chu](https://www.linkedin.com/in/kyle65463/).

----------------------
<br>
A little over a year ago I got pretty interested in training neural networks that can learn physics from data. Around the time machine learning interatomic potential (MLIP) models were becoming popular. These models broadly take as input a material's atomic configuration and predict properties related to its potential energy. All the papers I was reading had great results, but I felt they were selling an incomplete story because they were missing scaling information. To me, understanding how a model scales is perhaps the most important factor and having just 1 datapoint of a final test loss was insufficient to understand how any architecture would stand the test of time. 

We tried to investigate whether vanilla transformer encoders, given sufficient data, could learn to predict material properties as well as architectures explicitly equivariant architectures, like EquiformerV2 (EqV2) [[1]](https://arxiv.org/pdf/2306.12059). Inspired by the Bitter Lesson [[2]](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf), my hypothesis was that that transformers would scale more slowly than specialized architectures but that their scaling laws [[3]](https://arxiv.org/pdf/2001.08361)[[4]](https://arxiv.org/pdf/1712.00409)[[5]](https://arxiv.org/pdf/2210.16859) would hold out over more orders-of-magnitude (OOM).

We failed to stably train transformers most likely due to the lack of pretraining and featurization I chose. I still learned a great deal along the way, and found power laws for EqV2.

<br>
**Why the math makes sense:**

A Transformer is a graph neural network (GNN) on a complete graph with learned edge weights [[6]](https://arxiv.org/pdf/1704.01212). A graph in a GNN is created through a rule, either a known relation between nodes i.e. this paper cites another or a cutoff i.e. this atom is too far from the other so we assume they won't interact.

<div class="desktop-only">
  <iframe id="attentionFrame" src="{{site.url}}/assets/scaling/attention-graph.html"
          width="100%" style="border:0; border-radius:8px; box-shadow:0 2px 8px rgba(0,0,0,.1); background:#fff"></iframe>
  <script>
    const f = document.getElementById('attentionFrame');
    f.addEventListener('load', () => {
      const doc = f.contentDocument || f.contentWindow.document;
      f.style.height = doc.documentElement.scrollHeight + 'px';
    });
  </script>
</div>

<div class="mobile-only">
  <figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 100%;">
        <img src="{{site.url}}/assets/scaling/attention-graph.jpeg" alt="attention graph"/>
    </div>
  </figure>
</div>


<br>
A simple argument between the two architectures is that they fall into the bias - expressivity tradeoff. My take is that since self-attention on a fully connected graph is mathematically equivalent to message passing [[6]](https://arxiv.org/pdf/1704.01212) it should be able to learn weights between atoms without having to describe them.

<br>
**Task and Dataset**

MLIPs are trained to take in a set of atoms and their positions to predict the structure's energy and typically the forces on the atoms as well as the structure's stresses.

A common criticism of MLIPs trained on Density Functional Theory (DFT) calculations is that those datasets are relaxed structures around 0K. This means that they're not physically relevant in most cases to us because we don't live around 0K.

The Open Materials 2024 (OMat24) [[7]](https://huggingface.co/datasets/facebook/OMAT24) dataset is a 110 million crystal structure dataset that addresses this problem by focusing on including non-equilibrium (high temperature, structurally perturbed) samples. It's also one of the largest datasets of its kind.

----------------------

<br>
**Sin #1: Transformer featurization**

In my attempt at training transformers I wanted to use as few inductive biases as possible. i.e. no equivariant features, no invariant features, no periodic boundary conditions. This was an attempt to learn everything from the data + augmentation, regardless of sample efficiency.

I used a standard embedding layer for atom types to give the model a dictionary lookup of what each atom is across different structures. This was important because the model needed to understand that each atom is the same in different structures but is modified by its context, similar to how each word is the same in different sequences and is modified by its context. The 3D positions were concatenated with the embedding vector because I thought the model might have an easier time disentangling meaning vs. saving parameters by adding the positions to the embeddings. 

<div class="desktop-only" markdown="1">
$$
\begin{array}{c@{}c@{}c}
\text{Input feature matrix: } &
\left[
\begin{array}{ccc|ccc}
e_{11} & \cdots & e_{1d} & x_1 & y_1 & z_1 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
e_{N1} & \cdots & e_{Nd} & x_N & y_N & z_N
\end{array}
\right] &
\in \mathbb{R}^{N\times(d+3)}
\\[-2ex]
& \begin{array}{c@{\mkern8mu}c}
\underbrace{\hphantom{e_{11}\ \cdots\ e_{1d}}}_{\text{atomic embeddings}} &
\underbrace{\hphantom{x_1\ y_1\ z_1}}_{\text{positional encoding}}
\end{array}
\end{array}
$$
</div>

<div class="mobile-only" markdown="1">
$$
\begin{array}{l}
\text{Input feature matrix:} \\[2ex]
\left[
\begin{array}{ccc|ccc}
e_{11} & \cdots & e_{1d} & x_1 & y_1 & z_1 \\
\vdots & \ddots & \vdots & \vdots & \vdots & \vdots \\
e_{N1} & \cdots & e_{Nd} & x_N & y_N & z_N
\end{array}
\right]
\\[-2ex]
\mkern20mu\underbrace{\hphantom{e_{11}\ \cdots\ e_{1d}}}_{\text{atomic embeddings}}
\mkern24mu
\underbrace{\hphantom{x_1\ y_1\ z_1}}_{\text{positional encoding}}
\end{array}
$$
</div>



This was purposefully not a rotationally invariant featurization as I wanted to see if the model could learn this through augmentation. It also did not account for the fact crystal structures are periodic, which means that forces on atoms can come from adjacent cells. My findings are that this featurization led to the model learning global structure energy and stresses well, but not 3D per-atom forces. This isn't to say that forces weren't learned at all, but they were certainly not comparable to EquiformerV2.

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/train-force-loss.png" alt="transformer train force loss sweep"/>
        <figcaption>Sweeping 1M parameter transformers on 100k structures</figcaption>
    </div>
    <br>
</figure>

I got stuck with this phenomena of an apparent plateau in force loss only for certain hyperparam configs. There were other runs that would break through but still plateaued much higher than the EqV2. I suspect that training stability had a role with the former, and ultimately the lack of periodic boundary conditions impacting the latter.

However, I did notice the transformers were learning physically meaningful attention patterns without it being an explicit task. 

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/distance_vs_attention.png" alt="transformers learn graph structure"/>
        <!-- <figcaption></figcaption> -->
    </div>
    <br>
</figure>

Inspecting the first layer and head of the transformer shows a learned inverse relationship between the attention weights and interatomic distances, which is physically correct. This overcomes the inflexibility of a GNN's graph cutoff. 


Tough lessons learned:
- Transformers probably would have performed better if pretrained to learn graph structure and then finetuned to predict material properties
- The models just want to learn and given enough parameters the loss will go down even if they aren't learning the right thing.
- If scaling behavior doesn't appear in smaller OOMs it's unlikely it will magically appear later

<br>
**Sin #2: Not starting with individual experiments**

Instead, I rushed to create a more complex [set of scripts](https://github.com/akshaytrikha/materials-scaling) that would automatically run scaling experiments over multiple OOMs. I came up with what I thought was a clever way of iterating through models:

<!-- ```Python
class MetaTransformerModels:
    def __init__(
        self,
        vocab_size,
        max_seq_len,
    ):
        """Initializes TransformerModels with a list of configurations"""
        self.configurations = [
            {"d_model": 2, "depth": 1, "n_heads": 1, "d_ff_mult": 4}, # 2,280 params
            ...
            {"d_model": 256, "depth": 3, "n_heads": 2, "d_ff_mult": 4}, # 2,414,000 params
        ]
        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len

    def __getitem__(self, idx):
        """Retrieves transformer model corresponding to the configuration at idx"""
        config = self.configurations[idx]

        return XTransformerModel(
            num_tokens=self.vocab_size,
            d_model=config["d_model"],
            depth=config["depth"],
            n_heads=config["n_heads"],
            d_ff_mult=config["d_ff_mult"],
        )

    def __len__(self):
        return len(self.configurations)

    def __iter__(self):
        for idx in range(len(self.configurations)):
            yield self[idx]
``` -->

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/MetaTransformerModels.png" alt="MetaTransformerModels"/>
    </div>
    <br>
</figure>

On paper this sounds great to iterate over model sizes and lazily instantiate them, but in practice each OOM brings nuance and unexpectedness in behavior through e.g. hyperparameter sensitivity like early stoping. Instead, I should've started with small manual overfitting experiments and gradually increasing the parameters and data [[8]](http://karpathy.github.io/2019/04/25/recipe/). 

<br>
**Sin #3: Not starting with a small, in-memory, dataset**

Over all the experiments I ran I found that dataloading was typically the bottleneck in training time. This is an example of a bad dataset class I wrote. The devil is in the details because the `AseDBDataset.get_atoms(idx)` call looks like a simple getter but is actually doing disk I/O.

<div style="display: grid; grid-template-columns: 1.2fr 1fr; gap: 20px; margin: 20px 0;">
<style>
  .code-comparison pre,
  .code-comparison code {
    font-size: 0.85em;
  }
</style>
<div markdown="1" class="code-comparison">

**Bad approach ❌**

<img src="{{site.url}}/assets/scaling/bad-approach.png" alt="Bad approach code"/>

The result is that all of this work repeats every call:
- With random shuffling causing worst-case random disk access
- Across multiple worker processes (each with their own DB connections)

This is painful.

</div>
<div markdown="1" class="code-comparison">

**Better approach ✅**

<img src="{{site.url}}/assets/scaling/better-approach.png" alt="Better approach code"/>

A very simple solution is to start experimenting with a very small dataset and iterate through it completely to cache it before training.

</div>
</div> 


At a small dataset scale the first approach didn't matter. But it wasted a lot of time as I scaled the dataset size.

----------------------

<br>
**Win #1: Some EquiformerV2 results**

<div class="eqv2-results-container">
  <div class="result-img">
    <img src="{{site.url}}/assets/scaling/eqv2-param-scaling.png" alt="EquiformerV2 Parameter Scaling">
  </div>
  <div class="result-img">
    <img src="{{site.url}}/assets/scaling/eqv2-compute-scaling.png" alt="EquiformerV2 Compute Scaling">
  </div>
  <div class="result-img">
    <img src="{{site.url}}/assets/scaling/eqv2-dataset-scaling.png" alt="EquiformerV2 Dataset Scaling">
  </div>
</div>

<style>
.eqv2-results-container{
  /* full width breakout */
  width: 100vw;
  max-width: 100vw;
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);

  /* 3-up grid */
  padding-inline: 2rem;           /* optional gutter */
  box-sizing: border-box;
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 2rem;
  align-items: stretch;
}

.result-img{ 
  width:100%; 
  margin:0;
  display: flex;
  align-items: center;
  justify-content: center;
}

.result-img img{
  width: 100%;
  height: 350px;
  object-fit: contain;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

/* responsive wrap-downs */
@media (max-width: 1200px){ .eqv2-results-container{ grid-template-columns: repeat(2, 1fr); } }
@media (max-width: 768px){  .eqv2-results-container{ grid-template-columns: 1fr; } }
</style>

<br>
Takeaways:
- Demonstrated power laws
- Data scaling shows diminishing returns compared to param scaling
- Identified compute-optimal training configs with a real pareto frontier
- Data and training pipeline worked for EqV2, so there was something inherently wrong with the transformer

<br>
**Win #2: Making an inference visualization tool**

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/inference.gif" alt="inference visualization"/>
    </div>
</figure>

This ended up being a very useful debugging tool to understand how the models were behaving. For example as a sanity check it was nice to see the transformer wasn't only predicting 0 forces nor the mean of the dataset, and also that models were learning smaller magnitude forces as well as larger ones.

<br>
**Win #3: Making a scaling law experiment run tool**

<style>
.interactive-plots-container{
  /* full width breakout */
  width: 100vw;
  max-width: 100vw;
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);

  /* keep your 3-up grid */
  padding-inline: 2rem;           /* optional gutter */
  box-sizing: border-box;
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 2rem;
  align-items: stretch;
}

/* keep these from Option A */
.plot-dual-container{ display: contents; }
.plot-main, .plot-secondary{ width:100%; height:400px; margin:0; }

/* responsive wrap-downs */
@media (max-width: 1200px){ .interactive-plots-container{ grid-template-columns: repeat(2, 1fr); } }
@media (max-width: 768px){  .interactive-plots-container{ grid-template-columns: 1fr; } }
</style>

<div class="desktop-only" markdown="1"> 
<div class="interactive-plots-container">
  <div id="eqv2-scaling-plot1" class="plot-main"></div>
  <div class="plot-dual-container">
    <div id="eqv2-scaling-plot2" class="plot-secondary"></div>
    <div id="eqv2-scaling-plot3" class="plot-secondary"></div>
  </div>
</div>
</div>

<div class="mobile-only">
  <figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 100%;">
        <img src="{{site.url}}/assets/scaling/interactive-plots.gif" alt="interactive scaling plots"/>
    </div>
  </figure>
</div>


<!-- Plotly -->
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
<!-- Load your module and render -->
<script type="module">
  import { renderEqV2Scaling } from "{{ '/assets/scaling/eqv2-scaling.js' | relative_url }}";
  renderEqV2Scaling('eqv2-scaling-plot1', 'eqv2-scaling-plot2', 'eqv2-scaling-plot3');
</script>

This tool helped group families of runs and studying their behavior on the same plot. Before training any models I established naive (no deep learning) baselines that would help understand the loss number. The three baselines were:
- Loss while predicting 0 everywhere
- Loss while predicting the mean of the dataset everywhere
- Loss from a k = 1 nearest-neighbor Markov model

<br>
**Win #4: Getting to talk to John Jumper about scaling AlphaFold**

I think my life peaked a little when I got to have a pizza and a beer with John Jumper at the YC AI Startup School and pitch him this unfinished research. 

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/john-jumper.jpg" alt="meeting John Jumper"/>
        <figcaption>I met one of my heroes and he turned out to be extremely kind.</figcaption>
    </div>
    <br>
</figure>


#### References:

- [1] [EquiformerV2](https://arxiv.org/pdf/2306.12059) - Liao et al., 2023
- [2] [The Bitter Lesson](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf) - Richard Sutton, 2019
- [3] [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361) - Kaplan et al., 2020
- [4] [Deep Learning Scaling Is Predictable, Empirically](https://arxiv.org/pdf/1712.00409) - Hestness et al., 2017
- [5] [A Solvable Model of Neural Scaling Laws](https://arxiv.org/pdf/2210.16859) - Maloney et al., 2022
- [6] [Neural Message Passing for Quantum Chemistry](https://arxiv.org/pdf/1704.01212) - Gilmer et al., 2017
- [7] [Open Materials 2024 (OMat24) Dataset](https://huggingface.co/datasets/facebook/OMAT24) - Meta AI, 2024
- [8] [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/) - Andrej Karpathy, 2019
- [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/pdf/2203.15556) - Hoffmann et al., 2022
- [Naturally Occurring Equivariance in Neural Networks](https://distill.pub/2020/circuits/equivariance/) - Olah et al., 2020