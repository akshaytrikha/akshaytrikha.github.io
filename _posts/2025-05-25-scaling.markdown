---
layout: post
title: "Lessons in Scaling Machine Learning Interatomic Potentials"
date: 2025-09-25 13:11:17 -0000
categories: deep-learning
---

*Or, Why My Master's Thesis Failed*

**TL;DR:** I tested whether a plain transformer (no equivariance, no periodic boundary conditions) can learn interatomic forces as data & params scale vs. EquiformerV2. EqV2 followed clean power laws and hit much lower loss. My transformer failed and plataued hard, leading me to believe my featurization was insufficient. I share the wins & failures.

----------------------
<br>
A little over a year ago I got pretty interested in training neural networks that can learn physics from data. Around the time machine learning interatomic potential (NNIP) models were becoming popular. These models broadly take as input a material's atomic configuration and predict properties related to its potential energy. All the papers I was reading had great results, but I felt they were selling an incomplete story because they were missing scaling information. To me, understanding how a model scales is perhaps the most important thing and just having 1 datapoint of a final test loss was insufficent to understand how any architecture would stand the test of time. 

I tried to investigate whether vanilla transformers, given sufficient data, could learn to predict material properties as well as architectures explicitly equivariant architectures, like EquiformerV2 (EqV2) [[1]](https://arxiv.org/pdf/2306.12059). Inspired by the Bitter Lesson [[2]](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf), my hypothesis was that that transformers would scale more slowly than specialized architectures but that their scaling laws [[3]](https://arxiv.org/pdf/2001.08361)[[4]](https://arxiv.org/pdf/1712.00409)[[5]](https://arxiv.org/pdf/2210.16859) would hold out over more orders-of-magntiude (OOM).

I failed to stably train transformers most likely due to the featurization I chose. I still learned a great deal along the way, and found power laws for EqV2.

<br>
**Why the math makes sense:**

A Transformer is a graph neural network (GNN) on a complete graph with learned edge weights [[6]](https://arxiv.org/pdf/1704.01212). A graph in a GNN is created through a rule, either a known relation between nodes i.e. this paper cites another or a cutoff i.e. this atom is too far from the other so we assume they won't interact.

<iframe id="attentionFrame" src="{{site.url}}/assets/scaling/attention-graph.html"
        width="100%" style="border:0; border-radius:8px; box-shadow:0 2px 8px rgba(0,0,0,.1); background:#fff"></iframe>
<script>
  const f = document.getElementById('attentionFrame');
  f.addEventListener('load', () => {
    const doc = f.contentDocument || f.contentWindow.document;
    f.style.height = doc.documentElement.scrollHeight + 'px';
  });
</script>


<br>
A simple argument between the two architectures is that they fall into the bias - expressivity tradeoff. My take is that since self-attention on a fully connected graph is mathematically equivalent to message passing [[6]](https://arxiv.org/pdf/1704.01212) it should be able to learn weights between atoms without having to describe them.

<br>
**Task and Dataset**

NNIPs are trained to take in a set atoms and their positions to predict the structure's energy and typically the forces on the atoms as well as the structure's stresses.

A common criticism of NNIPs trained on Density Functional Theory (DFT) calculations is that those datasets are relaxed structures around 0K. This means that they're not physically relevant in most cases to us because we don't live around 0K.

The Open Materials 2024 (OMat24) [[7]](https://huggingface.co/datasets/facebook/OMAT24) dataset is a 110 million crystal structure dataset that addresses this problem by focusing on including non-equilibrium (high temperature, structurally perturbed) samples. It's also one of the largest datasets of its kind.

----------------------

<br>
**Sin #1: Not starting with individual experiments**

Instead, I rushed to create a more complex [set of scripts](https://github.com/akshaytrikha/materials-scaling) that would automatically run scaling experiments over multiple OOMs. I came up with what I thought was a clever way of iterating through models:

<!-- ```Python
class MetaTransformerModels:
    def __init__(
        self,
        vocab_size,
        max_seq_len,
    ):
        """Initializes TransformerModels with a list of configurations"""
        self.configurations = [
            {"d_model": 2, "depth": 1, "n_heads": 1, "d_ff_mult": 4}, # 2,280 params
            ...
            {"d_model": 256, "depth": 3, "n_heads": 2, "d_ff_mult": 4}, # 2,414,000 params
        ]
        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len

    def __getitem__(self, idx):
        """Retrieves transformer model corresponding to the configuration at idx"""
        config = self.configurations[idx]

        return XTransformerModel(
            num_tokens=self.vocab_size,
            d_model=config["d_model"],
            depth=config["depth"],
            n_heads=config["n_heads"],
            d_ff_mult=config["d_ff_mult"],
        )

    def __len__(self):
        return len(self.configurations)

    def __iter__(self):
        for idx in range(len(self.configurations)):
            yield self[idx]
``` -->

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/MetaTransformerModels.png" alt="MetaTransformerModels"/>
    </div>
    <br>
</figure>

On paper this sounds great to iterate over model sizes and lazily instantiate them, but in practice each OOM brings nuance and unexpectedness in behavior through e.g. hyperparameter sensitivity like early stoping. Instead, I should've started with small manual overfitting experiments and gradually increasing the parameters and data [[8]](http://karpathy.github.io/2019/04/25/recipe/). 

<br>
**Sin #2: Not starting with a small, in-memory, dataset**

Over all the experiments I ran I found that dataloading was typically the bottleneck in training time. This is an example of a bad dataset clase I wrote. The devil is in the details because the the `AseDBDataset.get_atoms(idx)` call looks like a simple getter but is actually doing disk I/O.

<div style="display: grid; grid-template-columns: 1.2fr 1fr; gap: 20px; margin: 20px 0;">
<style>
  .code-comparison pre,
  .code-comparison code {
    font-size: 0.85em;
  }
</style>
<div markdown="1" class="code-comparison">

**Bad approach ❌**

<img src="{{site.url}}/assets/scaling/bad-approach.png" alt="Bad approach code"/>

The result is that all of this work repeats every call:
- With random shuffling causing worst-case random disk access
- Accross multiple worker processes (each with their own DB connections)

This is painful.

</div>
<div markdown="1" class="code-comparison">

**Better approach ✅**

<img src="{{site.url}}/assets/scaling/better-approach.png" alt="Better approach code"/>

A very simple solution is to start experimenting with a very small dataset and iterate through it completely before training to cache it.

</div>
</div> 


At a small dataset scale the first approach didn't matter. But it wasted a lot of time as I scaled my dataset size.


<br>
**Sin #3: Transformer featurization**

In my attempt at training transformers I wanted to use as few inductive biases as possible. i.e. no equivariant features, no invariant features, no periodic boundary conditions. This was an attempt to learn everything from the data + augmentation, regardless of sample efficiency.

I used a standard embedding layer for atom types to give the model a dictionary lookup of what each atom is across different structures. This was important because the model needed to understand that each atom is the same in different in every structure but gets modified by its context, similar to how each word is the same in different sequences and is modified by its context. The 3D positions were concatenated to the embedding vector because I thought the model might have an easier time disentangling meaning vs. saving parameters by adding the positions to the embeddings. 

This was purposefully not a rotationally invariant featurization as I wanted to see if the model could learn this through augmentation. It also did not account for the fact crystal structures are periodic, which means that forces on atoms can come from adjacent cells. My findings are that this featurization led to the model learning global structure energy and stresses well, but not 3D per-atom forces. It's not to say that forces weren't learned at all, but they were certainly not comparable to the EquiformerV2.

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/train-force-loss.png" alt="transformer train force loss sweep"/>
        <figcaption>Sweeping 1M parameter transformers on 100k structures</figcaption>
    </div>
    <br>
</figure>

I got stuck with this phenomena of an apparent plateau in force loss only for certain hyperparam configs. There were other runs that would break through but still plateau much higher than the EqV2. I suspect that training stability had a role with the former, and ultimately the lack of periodic boundary conditions impacting the latter.

Tough lessons learned:
- The models just want to learn and given enough parameters the loss will go down even if they aren't learning the right thing.
- If scaling behavior doesn't appear in smaller OOMs it's unlikely it will magically appear later

----------------------

<br>
**Win #1: Some EquiformerV2 results**

<div class="eqv2-results-container">
  <div class="result-img">
    <img src="{{site.url}}/assets/scaling/eqv2-param-scaling.png" alt="EquiformerV2 Parameter Scaling">
  </div>
  <div class="result-img">
    <img src="{{site.url}}/assets/scaling/eqv2-compute-scaling.png" alt="EquiformerV2 Compute Scaling">
  </div>
  <div class="result-img">
    <img src="{{site.url}}/assets/scaling/eqv2-dataset-scaling.png" alt="EquiformerV2 Dataset Scaling">
  </div>
</div>

<style>
.eqv2-results-container{
  /* full width breakout */
  width: 100vw;
  max-width: 100vw;
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);

  /* 3-up grid */
  padding-inline: 2rem;           /* optional gutter */
  box-sizing: border-box;
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 2rem;
  align-items: stretch;
}

.result-img{ 
  width:100%; 
  margin:0;
  display: flex;
  align-items: center;
  justify-content: center;
}

.result-img img{
  width: 100%;
  height: 350px;
  object-fit: contain;
  border-radius: 8px;
  box-shadow: 0 2px 8px rgba(0,0,0,0.1);
}

/* responsive wrap-downs */
@media (max-width: 1200px){ .eqv2-results-container{ grid-template-columns: repeat(2, 1fr); } }
@media (max-width: 768px){  .eqv2-results-container{ grid-template-columns: 1fr; } }
</style>

<br>
Takeaways:
- Demonstrated power laws
- Data scaling shows diminishing returns compared to param scaling
- Identified compute-optimal training configs with a real pareto frontier
- My data and training pipeline worked for EqV2, so there was something inherently wrong with the transformer

<br>
**Win #2: Making an inference visualization tool**

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/inference.gif" alt="inference visualization"/>
    </div>
    <br>
</figure>

This ended up being a very useful debugging tool to understand how the models were behaving. For example as a sanity check it was nice to see the transformer wasn't only predicting 0 forces nor the mean of the dataset, and also that models were learning smaller magnitude forces as well as larger ones.

<br>
**Win #3: Making a scaling law experiment run tool**

<div class="scaling-plots-container">
  <div id="eqv2-scaling-plot1" class="plot-main"></div>
  <div class="plot-dual-container">
    <div id="eqv2-scaling-plot2" class="plot-secondary"></div>
    <div id="eqv2-scaling-plot3" class="plot-secondary"></div>
  </div>
</div>

<style>
.scaling-plots-container{
  /* full width breakout */
  width: 100vw;
  max-width: 100vw;
  margin-left: calc(50% - 50vw);
  margin-right: calc(50% - 50vw);

  /* keep your 3-up grid */
  padding-inline: 2rem;           /* optional gutter */
  box-sizing: border-box;
  display: grid;
  grid-template-columns: repeat(3, minmax(0, 1fr));
  gap: 2rem;
  align-items: stretch;
}

/* keep these from Option A */
.plot-dual-container{ display: contents; }
.plot-main, .plot-secondary{ width:100%; height:400px; margin:0; }

/* responsive wrap-downs */
@media (max-width: 1200px){ .scaling-plots-container{ grid-template-columns: repeat(2, 1fr); } }
@media (max-width: 768px){  .scaling-plots-container{ grid-template-columns: 1fr; } }
</style>


<!-- Plotly -->
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
<!-- Load your module and render -->
<script type="module">
  import { renderEqV2Scaling } from "{{ '/assets/scaling/eqv2-scaling.js' | relative_url }}";
  renderEqV2Scaling('eqv2-scaling-plot1', 'eqv2-scaling-plot2', 'eqv2-scaling-plot3');
</script>


<br>
**Win #4: Getting to talk to John Jumper about scaling AlphaFold**

I think my life peaked a little when I got to have a pizza and a beer with John Jumper at the YC AI Startup School and pitch him this unfinished research. 

I met one of my heros and he turned out to be extremely kind.




<!-- A Dummy’s Guide to Empirically Deriving Scaling Laws

(& What not to do)

1. Source compute first (assuming you already have a large dataset)
2. Do NOT start by YOLOing a huge run
    - Each run is only 1 data point so a large one is still only 1 data point
    - Stay small as long as possible
3. Your first goal is to overfit a model. Do this by following http://karpathy.github.io/2019/04/25/recipe/
4. Don’t forget to visualize & verify your outputs
5. Training efficiency bells & whistles
    1. FlashAttention, mixed precision gradient clipping,
-->



#### References:

- [1] [EquiformerV2](https://arxiv.org/pdf/2306.12059) - Liao et al., 2023
- [2] [The Bitter Lesson](https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf) - Richard Sutton, 2019
- [3] [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361) - Kaplan et al., 2020
- [4] [Deep Learning Scaling Is Predictable, Empirically](https://arxiv.org/pdf/1712.00409) - Hestness et al., 2017
- [5] [A Solvable Model of Neural Scaling Laws](https://arxiv.org/pdf/2210.16859) - Maloney et al., 2022
- [6] [Neural Message Passing for Quantum Chemistry](https://arxiv.org/pdf/1704.01212) - Gilmer et al., 2017
- [7] [Open Materials 2024 (OMat24) Dataset](https://huggingface.co/datasets/facebook/OMAT24) - Meta AI, 2024
- [8] [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/) - Andrej Karpathy, 2019
- [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/pdf/2203.15556) - Hoffmann et al., 2022
- [Naturally Occurring Equivariance in Neural Networks](https://distill.pub/2020/circuits/equivariance/) - Olah et al., 2020