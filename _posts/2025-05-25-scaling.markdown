---
layout: post
title: "Lessons in Scaling Neural Network Interatomic Potentials"
date: 2025-09-25 13:11:17 -0000
categories: deep-learning
---

*Or, Why My Master's Thesis Failed*

A little over a year ago I got pretty interested in training neural networks that can learn physics from data. I took an amazing class called [Physics Inspired Deep Learning](https://classes.berkeley.edu/content/2024-spring-compsci-294-254-lec-254) taught by Professor Aditi Krishnapriyan at Berkeley. I [wrote](http://localhost:4000/deep-learning/2024/03/11/darcy-flow.html) about one of the projects from the class where we forced a neural network to only learn solutions to partial differential equations of a certain form, which we knew to be correct from prior physics knowledge. 

Around the same time, neural network interatomic potential (NNIP) models were becoming popular. These models broadly take as input a material's atomic configuration and predict properties related to its potential energy. All the papers I was reading had great results, but I felt they were selling an incomplete story because they were missing scaling information. To me, understanding how a model scales is perhaps the most important thing and just having 1 datapoint of a final test loss was insufficent to understand how any architecture would stand the test of time. 

I tried to investigate whether vanilla transformers, given sufficient data, could learn to predict material properties as well as architectures explicitly equivariant architectures, like EquiformerV2 (EqV2). Inspired by the Bitter Lesson, my hypothesis was that that transformers would scale more slowly than specialized architectures but that their scaling laws would hold out over more orders-of-magntiude (OOM).


<!-- <div style="">
  <iframe
    src="{{site.url}}/assets/scaling/cartoon-scaling.html"
    width="900"
    height="600"
    style="border:0; overflow:hidden;"
    loading="lazy"
  ></iframe>
</div> -->



<div id="nnip-scaling"
  style="width:75%; max-width:900px; margin:auto; border:0;">
</div>




<!-- Over the last year I was writing my master's thesis.  -->

<br>
**Why the math makes sense:**

A Transformer is a graph neural network (GNN) on a complete graph with learned edge weights. A graph in a GNN is created through a rule, either a known relation between nodes i.e. this paper cites another or a cutoff i.e. this atom is too far from the other so we assume they won't interact.

<iframe id="attentionFrame" src="{{site.url}}/assets/scaling/attention-graph.html"
        width="100%" style="border:0; border-radius:8px; box-shadow:0 2px 8px rgba(0,0,0,.1); background:#fff"></iframe>
<script>
  const f = document.getElementById('attentionFrame');
  f.addEventListener('load', () => {
    const doc = f.contentDocument || f.contentWindow.document;
    f.style.height = doc.documentElement.scrollHeight + 'px';
  });
</script>


<br>
A simple argument between the two architectures is that they fall into the bias - expressivity tradeoff. My take is that since self-attention on a fully connected graph is mathematically equivalent to message passing it should be able to learn weights between atoms without having to describe them.

<br>
**Task and Dataset**

NNIPs are trained to take in a set atoms and their positions to predict the structure's energy and typically the forces on the atoms as well as the structure's stresses.

A common criticism of NNIPs trained on Density Functional Theory (DFT) calculations is that those datasets are relaxed structures around 0K. This means that they're not physically relevant in most cases to us because we don't live around 0K.

The [Open Materials 2024](https://huggingface.co/datasets/facebook/OMAT24) dataset is a 110 million crystal structure dataset that addresses this problem by focusing on including non-equilibrium (high temperature, structurally perturbed) samples. It's also one of the largest datasets of its kind.

<br>
**Sin #1: Not starting with individual experiments**

Instead, I rushed to create a more complex [set of scripts](https://github.com/akshaytrikha/materials-scaling) that would automatically run scaling experiments over multiple OOMs. I came up with what I thought was a clever way of iterating through models:

<!-- ```Python
class MetaTransformerModels:
    def __init__(
        self,
        vocab_size,
        max_seq_len,
    ):
        """Initializes TransformerModels with a list of configurations"""
        self.configurations = [
            {"d_model": 2, "depth": 1, "n_heads": 1, "d_ff_mult": 4}, # 2,280 params
            ...
        ]
        self.vocab_size = vocab_size
        self.max_seq_len = max_seq_len

    def __getitem__(self, idx):
        """Retrieves transformer model corresponding to the configuration at idx"""
        config = self.configurations[idx]

        return XTransformerModel(
            num_tokens=self.vocab_size,
            d_model=config["d_model"],
            depth=config["depth"],
            n_heads=config["n_heads"],
            d_ff_mult=config["d_ff_mult"],
        )

    def __len__(self):
        return len(self.configurations)

    def __iter__(self):
        for idx in range(len(self.configurations)):
            yield self[idx]
``` -->

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/MetaTransformerModels.png" alt="MetaTransformerModels"/>
    </div>
    <br>
</figure>

On paper this sounds great to iterate over model sizes and lazily instantiate them, but in practice each OOM brings nuance and unexpectedness in behavior through e.g. hyperparameter sensitivity like early stoping. Instead, I should've started with small manual overfitting experiments and gradually increasing the parameters and data. 


<!-- Insert MetaModelsClas -->

<br>
**Sin #2: Not starting with a small, in-memory, dataset**

Over all the experiments I ran I found that dataloading was typically the bottleneck in training time. This is an example of a bad dataset clase I wrote. The devil is in the details because the the `AseDBDataset.get_atoms(idx)` call looks like a simple getter but is actually doing disk I/O.

<div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0;">
<style>
  .code-comparison pre,
  .code-comparison code {
    font-size: 0.85em;
  }
</style>
<div markdown="1" class="code-comparison">

**Bad approach:**

```Python
def __getitem__(self, idx):
    # dataset is an AseDBDataset
    # lookup file → open SQLite → query → deserialize
    return self.dataset.get_atoms(idx) 
```

The result is that all of this work repeats every call:
- With random shuffling causing worst-case random disk access
- Accross multiple worker processes (each with their own DB connections)

This is painful.

</div>
<div markdown="1" class="code-comparison">

**Better approach:**

```Python
atoms_list = []

for idx in tqdm(range(len(ase_dataset))):
    atoms = ase_dataset.get_atoms(idx)
    atoms_list.append(atoms)

# Now __get_item__() is a list lookup in RAM
dataset = OMatDataset(atoms_list)
```

A very simple solution is to start experimenting with a very small dataset and iterate through it completely before training to cache it.

</div>
</div> 


At a small dataset scale the first approach didn't matter. But it wasted a lot of time as I scaled my dataset size.


<br>
**Sin #3: Transformer featurization**

In my attempt at training transformers I wanted to use as few inductive biases as possible. i.e. no equivariant features, no invariant features, no periodic boundary conditions. This was an attempt to learn everything from the data + augmentation, regardless of sample efficiency.

I used a standard embedding layer for atom types to give the model a dictionary lookup of what each atom is across different structures. This was important because the model needed to understand that each atom is the same in different in every structure but gets modified by its context, similar to how each word is the same in different sequences and is modified by its context. The 3D positions were concatenated to the embedding vector because I thought the model might have an easier time disentangling meaning vs. saving parameters by adding the positions to the embeddings. 

This was purposefully not a rotationally invariant featurization as I wanted to see if the model could learn this through augmentation. It also did not account for the fact crystal structures are periodic, which means that forces on atoms can come from adjacent cells. My findings are that this featurization led to the model learning global structure energy and stresses well, but not 3D per-atom forces. It's not to say that forces weren't learned at all, but they were certainly not comparable to the EquiformerV2.

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/train-force-loss.png" alt="transformer train force loss sweep"/>
        <figcaption>Sweeping 1M parameter transformers on 100k structures</figcaption>
    </div>
    <br>
</figure>

I got stuck with this phenomena of an apparent plateau in force loss only for certain hyperparam configs. There were other runs that would break through but still plateau much higher than the EqV2. I suspect that training stability had a role with the former, and ultimately the lack of periodic boundary conditions impacting latter.






<!-- Around the time I was stuck some researchers at FAIR published a paper using diffusion transformers to create a unified representation of both periodic and non-periodic systems (i.e. materials vs. molecules) and generate new structures. I thought their encoder was neat, and borrowed it for my task by adjusting its output heads. The main difference is that they also embedded the positions as well as fractional coordinates alongside the atom types and then added them all together in the forward pass. -->



<!-- Global energy and stresses were easy to learn, forces was where the instability was -->

<!-- Periodicity -->

<!-- Assuming I could feed the transformer just atom positions -->

<br>
**Win #0: Some EquiformerV2 results**

<br>
**Win #1: Making an inference visualization tool**

<figure style="display: flex; justify-content: center;">
    <div style="text-align: center; width: 80%;">
        <img src="{{site.url}}/assets/scaling/inference.gif" alt="inference visualization"/>
    </div>
    <br>
</figure>

<br>
**Win #4: Making an scaling law run tool**

<div class="scaling-plots-container">
  <div id="eqv2-scaling-plot1" class="plot-main"></div>
  <div class="plot-dual-container">
    <div id="eqv2-scaling-plot2" class="plot-secondary"></div>
    <div id="eqv2-scaling-plot3" class="plot-secondary"></div>
  </div>
</div>

<style>
.scaling-plots-container {
  width: 95%;
  max-width: 1600px;
  margin: 2rem auto;
  padding: 0 1rem;
}

.plot-main {
  width: 100%;
  height: 450px;
  margin-bottom: 2rem;
}

.plot-dual-container {
  position: relative;
  left: 50%;
  right: 50%;
  margin-left: -37.5vw;
  margin-right: -37.5vw;
  width: 75vw;
  max-width: 1400px;
  margin-top: 2rem;
  margin-bottom: 2rem;
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 2.5rem;
  padding: 0 2rem;
}

.plot-secondary {
  width: 100%;
  height: 400px;
}

@media (max-width: 768px) {
  .plot-dual-container {
    grid-template-columns: 1fr;
    width: 100%;
    left: 0;
    right: 0;
    margin-left: 0;
    margin-right: 0;
  }
  .plot-main, .plot-secondary {
    height: 400px;
  }
}
</style>

<!-- Plotly -->
<script src="https://cdn.plot.ly/plotly-2.35.2.min.js"></script>
<!-- Load your module and render -->
<script type="module">
  import { renderEqV2Scaling } from "{{ '/assets/scaling/eqv2-scaling.js' | relative_url }}";
  renderEqV2Scaling('eqv2-scaling-plot1', 'eqv2-scaling-plot2', 'eqv2-scaling-plot3');
</script>





<br>
**Win #5: Getting to talk to John Jumper about scaling Alphafold**







<!-- A Dummy’s Guide to Empirically Deriving Scaling Laws

(& What not to do)

1. Source compute first (assuming you already have a large dataset)
2. Do NOT start by YOLOing a huge run
    - Each run is only 1 data point so a large one is still only 1 data point
    - Stay small as long as possible
3. Your first goal is to overfit a model. Do this by following http://karpathy.github.io/2019/04/25/recipe/
4. Don’t forget to visualize & verify your outputs
5. Training efficiency bells & whistles
    1. FlashAttention, mixed precision gradient clipping,


References:
- A Recipe for Training Neural Networks http://karpathy.github.io/2019/04/25/recipe/ 
- Training Compute-Optimal Large Language Models (Chinchilla) https://arxiv.org/pdf/2203.15556 
- Scaling Laws for Neural Language Models https://arxiv.org/pdf/2001.08361  -->





<br>
**Learning #1: Source compute early on**

Through my PI, I got access to Berkeley's Savio which gave me ~55 days running an A5000. Nothing crazy but not small either. I also tried out RunPod for cheaper GPUs and Lambda Labs for when I was getting impatient and wanted a $1.49/hr H100. I didn't end up needing it, but I was considering applying to all the cloud companies for research credits [Azure](https://www.microsoft.com/en-us/azure-academic-research/#formMain ), [NVIDIA](https://academicgrants.nvidia.com/academicgrantprogram/s/welcome 
), [AWS](https://pages.awscloud.com/aws-cloud-credit-for-research.html ), [GCP](https://edu.google.com/programs/credits/research/?modal_active=none ), [Oracle](https://www.oracle.com/a/ocom/docs/research/project-award-application.pdf).

<br>
**Learning #2: Scale with Chinchilla's Hyperparam Config**


<br>
**Learning #3: The model's just want to learn**

A tough lesson I also learned that the models just want to learn and given enough parameters the loss will go 



#### References:

- [[1] EquiformerV2](https://arxiv.org/pdf/2306.12059)
- [[1] Symmetry From Scratch: Group Equivariance as a Supervised Learning Task](https://arxiv.org/html/2410.03989v1)
- [[2] Naturally Occurring Equivariance in Neural Networks](https://distill.pub/2020/circuits/equivariance/)
- [[3] All-atom Diffusion Transformers Encoder](https://github.com/facebookresearch/all-atom-diffusion-transformer/blob/main/src/models/encoders/transformer.py)
- [[4]]()
- [[5]]()




